{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import chemprop\n",
    "\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint = 'skin-sensitization'\n",
    "endpoint = 'eye-irritation'\n",
    "\n",
    "loc = r'D:\\School\\Semester3\\Seminar - Reproducibility\\seminar-toxicity\\data'\n",
    "endpoint_loc = os.path.join(loc, endpoint)\n",
    "model = r'D:\\School\\Semester3\\Seminar - Reproducibility\\seminar-toxicity\\src\\models'\n",
    "model_loc = os.path.join(model, endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Command line\n",
      "python c:\\Users\\kevin\\nextAID\\lib\\site-packages\\ipykernel_launcher.py --f=c:\\Users\\kevin\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-235886Vxa3DwY3TcH.json\n",
      "Args\n",
      "{'activation': 'ReLU',\n",
      " 'adding_bond_types': True,\n",
      " 'adding_h': False,\n",
      " 'aggregation': 'mean',\n",
      " 'aggregation_norm': 100,\n",
      " 'atom_constraints': [],\n",
      " 'atom_descriptor_scaling': True,\n",
      " 'atom_descriptors': None,\n",
      " 'atom_descriptors_path': None,\n",
      " 'atom_descriptors_size': 0,\n",
      " 'atom_features_size': 0,\n",
      " 'atom_messages': False,\n",
      " 'atom_targets': [],\n",
      " 'batch_size': 50,\n",
      " 'bias': False,\n",
      " 'bias_solvent': False,\n",
      " 'bond_constraints': [],\n",
      " 'bond_descriptor_scaling': True,\n",
      " 'bond_descriptors': None,\n",
      " 'bond_descriptors_path': None,\n",
      " 'bond_descriptors_size': 0,\n",
      " 'bond_features_size': 0,\n",
      " 'bond_targets': [],\n",
      " 'cache_cutoff': 10000,\n",
      " 'checkpoint_dir': None,\n",
      " 'checkpoint_frzn': None,\n",
      " 'checkpoint_path': None,\n",
      " 'checkpoint_paths': None,\n",
      " 'class_balance': False,\n",
      " 'config_path': 'D:\\\\School\\\\Semester3\\\\Seminar - '\n",
      "                'Reproducibility\\\\seminar-toxicity\\\\src\\\\models\\\\eye-irritation\\\\config.json',\n",
      " 'constraints_path': None,\n",
      " 'crossval_index_dir': None,\n",
      " 'crossval_index_file': None,\n",
      " 'crossval_index_sets': None,\n",
      " 'cuda': False,\n",
      " 'data_path': 'D:\\\\School\\\\Semester3\\\\Seminar - '\n",
      "              'Reproducibility\\\\seminar-toxicity\\\\data\\\\eye-irritation\\\\train.csv',\n",
      " 'data_weights_path': None,\n",
      " 'dataset_type': 'classification',\n",
      " 'depth': 3,\n",
      " 'depth_solvent': 3,\n",
      " 'device': device(type='cpu'),\n",
      " 'dropout': 0.15,\n",
      " 'empty_cache': False,\n",
      " 'ensemble_size': 5,\n",
      " 'epochs': 30,\n",
      " 'evidential_regularization': 0,\n",
      " 'explicit_h': False,\n",
      " 'extra_metrics': ['accuracy'],\n",
      " 'features_generator': ['rdkit_2d_normalized'],\n",
      " 'features_only': False,\n",
      " 'features_path': None,\n",
      " 'features_scaling': False,\n",
      " 'features_size': None,\n",
      " 'ffn_hidden_size': 1100,\n",
      " 'ffn_num_layers': 3,\n",
      " 'final_lr': 0.0001,\n",
      " 'folds_file': None,\n",
      " 'freeze_first_only': False,\n",
      " 'frzn_ffn_layers': 0,\n",
      " 'gpu': None,\n",
      " 'grad_clip': None,\n",
      " 'hidden_size': 1100,\n",
      " 'hidden_size_solvent': 300,\n",
      " 'ignore_columns': None,\n",
      " 'init_lr': 0.0001,\n",
      " 'is_atom_bond_targets': False,\n",
      " 'keeping_atom_map': False,\n",
      " 'log_frequency': 10,\n",
      " 'loss_function': 'binary_cross_entropy',\n",
      " 'max_data_size': None,\n",
      " 'max_lr': 0.001,\n",
      " 'metric': 'auc',\n",
      " 'metrics': ['auc', 'accuracy'],\n",
      " 'minimize_score': False,\n",
      " 'mpn_shared': False,\n",
      " 'multiclass_num_classes': 3,\n",
      " 'no_adding_bond_types': False,\n",
      " 'no_atom_descriptor_scaling': False,\n",
      " 'no_bond_descriptor_scaling': False,\n",
      " 'no_cache_mol': False,\n",
      " 'no_cuda': False,\n",
      " 'no_features_scaling': True,\n",
      " 'no_shared_atom_bond_ffn': False,\n",
      " 'num_folds': 1,\n",
      " 'num_lrs': 1,\n",
      " 'num_tasks': 1,\n",
      " 'num_workers': 8,\n",
      " 'number_of_molecules': 1,\n",
      " 'overwrite_default_atom_features': False,\n",
      " 'overwrite_default_bond_features': False,\n",
      " 'phase_features_path': None,\n",
      " 'pytorch_seed': 0,\n",
      " 'quiet': False,\n",
      " 'reaction': False,\n",
      " 'reaction_mode': 'reac_diff',\n",
      " 'reaction_solvent': False,\n",
      " 'resume_experiment': False,\n",
      " 'save_dir': 'D:\\\\School\\\\Semester3\\\\Seminar - '\n",
      "             'Reproducibility\\\\seminar-toxicity\\\\src\\\\models\\\\eye-irritation',\n",
      " 'save_preds': False,\n",
      " 'save_smiles_splits': False,\n",
      " 'seed': 0,\n",
      " 'separate_test_atom_descriptors_path': None,\n",
      " 'separate_test_bond_descriptors_path': None,\n",
      " 'separate_test_constraints_path': None,\n",
      " 'separate_test_features_path': None,\n",
      " 'separate_test_path': 'D:\\\\School\\\\Semester3\\\\Seminar - '\n",
      "                       'Reproducibility\\\\seminar-toxicity\\\\data\\\\eye-irritation\\\\val.csv',\n",
      " 'separate_test_phase_features_path': None,\n",
      " 'separate_val_atom_descriptors_path': None,\n",
      " 'separate_val_bond_descriptors_path': None,\n",
      " 'separate_val_constraints_path': None,\n",
      " 'separate_val_features_path': None,\n",
      " 'separate_val_path': None,\n",
      " 'separate_val_phase_features_path': None,\n",
      " 'shared_atom_bond_ffn': True,\n",
      " 'show_individual_scores': False,\n",
      " 'smiles_columns': ['SMILES'],\n",
      " 'spectra_activation': 'exp',\n",
      " 'spectra_phase_mask_path': None,\n",
      " 'spectra_target_floor': 1e-08,\n",
      " 'split_key_molecule': 0,\n",
      " 'split_sizes': [0.8, 0.2, 0.0],\n",
      " 'split_type': 'random',\n",
      " 'target_columns': ['Activity'],\n",
      " 'target_weights': None,\n",
      " 'task_names': ['Activity'],\n",
      " 'test': False,\n",
      " 'test_fold_index': None,\n",
      " 'train_data_size': None,\n",
      " 'undirected': False,\n",
      " 'use_input_features': True,\n",
      " 'val_fold_index': None,\n",
      " 'warmup_epochs': 2.0,\n",
      " 'weights_ffn_num_layers': 2}\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n",
      "3101it [00:00, 196529.77it/s]\n",
      " 23%|██▎       | 712/3101 [01:03<04:52,  8.17it/s][18:18:29] Can't kekulize mol.  Unkekulized atoms: 0 1 2 3 4\n",
      " 37%|███▋      | 1141/3101 [01:55<03:38,  8.97it/s][18:19:22] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 14 15 16 18 19 20 21 22 23\n",
      " 58%|█████▊    | 1808/3101 [02:42<01:15, 17.19it/s][18:20:08] Can't kekulize mol.  Unkekulized atoms: 5 6 7 9 10 11 12 13 14 15 16 17 18\n",
      " 65%|██████▌   | 2020/3101 [02:54<00:57, 18.94it/s][18:20:20] Can't kekulize mol.  Unkekulized atoms: 5 6 7 9 10 11 12 13 14 15 16 17 18\n",
      " 68%|██████▊   | 2105/3101 [02:58<00:40, 24.38it/s][18:20:24] Can't kekulize mol.  Unkekulized atoms: 10 11 12 13 15 16 17 19 20 21 25 27 28\n",
      "100%|██████████| 3101/3101 [03:39<00:00, 14.13it/s]\n",
      "100%|██████████| 3101/3101 [00:00<00:00, 148557.85it/s]\n",
      "Warning: 5 SMILES are invalid.\n",
      "Number of tasks = 1\n",
      "Fold 0\n",
      "Splitting data with seed 0\n",
      "776it [00:00, 234494.23it/s]\n",
      " 76%|███████▌  | 589/776 [00:23<00:07, 23.77it/s][18:21:29] Can't kekulize mol.  Unkekulized atoms: 1 2 3 4 5 6 7 9 10 11 13 14 15\n",
      "100%|██████████| 776/776 [00:31<00:00, 24.78it/s]\n",
      "100%|██████████| 776/776 [00:00<00:00, 166749.32it/s]\n",
      "Warning: 1 SMILES are invalid.\n",
      "Class sizes\n",
      "Activity 0: 31.65%, 1: 68.35%\n",
      "Total size = 3,096 | train size = 2,476 | val size = 620 | test size = 775\n",
      "Building model 0\n",
      "MoleculeModel(\n",
      "  (sigmoid): Sigmoid()\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.15, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=1100, bias=False)\n",
      "        (W_h): Linear(in_features=1100, out_features=1100, bias=False)\n",
      "        (W_o): Linear(in_features=1233, out_features=1100, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.15, inplace=False)\n",
      "    (1): Linear(in_features=1300, out_features=1100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.15, inplace=False)\n",
      "    (4): Linear(in_features=1100, out_features=1100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.15, inplace=False)\n",
      "    (7): Linear(in_features=1100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 5,373,501\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Epoch 0\n",
      "Loss = 6.8475e-01, PNorm = 69.4082, GNorm = 0.8795, lr_0 = 2.0102e-04\n",
      "Loss = 6.1493e-01, PNorm = 69.4269, GNorm = 0.5741, lr_0 = 2.9286e-04\n",
      "Loss = 5.7657e-01, PNorm = 69.4532, GNorm = 0.5886, lr_0 = 3.8469e-04\n",
      "Loss = 5.8754e-01, PNorm = 69.4818, GNorm = 0.7496, lr_0 = 4.7653e-04\n",
      "Validation auc = 0.757548\n",
      "Validation accuracy = 0.712903\n",
      "  3%|▎         | 1/30 [00:35<17:02, 35.25s/it]Epoch 1\n",
      "Loss = 6.0865e-01, PNorm = 69.5259, GNorm = 0.8758, lr_0 = 5.7755e-04\n",
      "Loss = 5.6442e-01, PNorm = 69.5803, GNorm = 0.5296, lr_0 = 6.6939e-04\n",
      "Loss = 5.6642e-01, PNorm = 69.6477, GNorm = 0.5653, lr_0 = 7.6122e-04\n",
      "Loss = 6.1680e-01, PNorm = 69.7254, GNorm = 1.1599, lr_0 = 8.5306e-04\n",
      "Loss = 5.7004e-01, PNorm = 69.8181, GNorm = 0.6816, lr_0 = 9.4490e-04\n",
      "Validation auc = 0.784845\n",
      "Validation accuracy = 0.725806\n",
      "  7%|▋         | 2/30 [01:06<15:19, 32.82s/it]Epoch 2\n",
      "Loss = 5.3848e-01, PNorm = 69.9170, GNorm = 0.6194, lr_0 = 9.9331e-04\n",
      "Loss = 5.6061e-01, PNorm = 70.0092, GNorm = 0.7128, lr_0 = 9.7678e-04\n",
      "Loss = 5.5121e-01, PNorm = 70.0942, GNorm = 0.6484, lr_0 = 9.6052e-04\n",
      "Loss = 5.2353e-01, PNorm = 70.1878, GNorm = 0.7278, lr_0 = 9.4454e-04\n",
      "Loss = 5.8607e-01, PNorm = 70.2682, GNorm = 0.5485, lr_0 = 9.2882e-04\n",
      "Validation auc = 0.794429\n",
      "Validation accuracy = 0.722581\n",
      " 10%|█         | 3/30 [01:35<14:04, 31.28s/it]Epoch 3\n",
      "Loss = 5.9315e-01, PNorm = 70.3454, GNorm = 0.4488, lr_0 = 9.1183e-04\n",
      "Loss = 5.0679e-01, PNorm = 70.4238, GNorm = 1.1093, lr_0 = 8.9665e-04\n",
      "Loss = 4.9554e-01, PNorm = 70.5104, GNorm = 0.7783, lr_0 = 8.8173e-04\n",
      "Loss = 5.0989e-01, PNorm = 70.5843, GNorm = 0.5370, lr_0 = 8.6706e-04\n",
      "Loss = 5.3645e-01, PNorm = 70.6493, GNorm = 0.4234, lr_0 = 8.5262e-04\n",
      "Validation auc = 0.781071\n",
      "Validation accuracy = 0.735484\n",
      " 13%|█▎        | 4/30 [02:05<13:21, 30.82s/it]Epoch 4\n",
      "Loss = 4.4451e-01, PNorm = 70.7092, GNorm = 0.4612, lr_0 = 8.3843e-04\n",
      "Loss = 5.1205e-01, PNorm = 70.7744, GNorm = 0.5152, lr_0 = 8.2448e-04\n",
      "Loss = 4.7829e-01, PNorm = 70.8386, GNorm = 0.6466, lr_0 = 8.1076e-04\n",
      "Loss = 4.9465e-01, PNorm = 70.9137, GNorm = 0.9655, lr_0 = 7.9727e-04\n",
      "Loss = 4.7188e-01, PNorm = 70.9985, GNorm = 0.5566, lr_0 = 7.8400e-04\n",
      "Validation auc = 0.787988\n",
      "Validation accuracy = 0.746774\n",
      " 17%|█▋        | 5/30 [02:37<12:58, 31.15s/it]Epoch 5\n",
      "Loss = 4.6965e-01, PNorm = 71.0869, GNorm = 0.6500, lr_0 = 7.6966e-04\n",
      "Loss = 4.6678e-01, PNorm = 71.1610, GNorm = 0.7386, lr_0 = 7.5685e-04\n",
      "Loss = 4.6934e-01, PNorm = 71.2358, GNorm = 1.0613, lr_0 = 7.4425e-04\n",
      "Loss = 4.9104e-01, PNorm = 71.3063, GNorm = 0.6216, lr_0 = 7.3187e-04\n",
      "Loss = 4.7561e-01, PNorm = 71.3752, GNorm = 0.6888, lr_0 = 7.1969e-04\n",
      "Validation auc = 0.801167\n",
      "Validation accuracy = 0.764516\n",
      " 20%|██        | 6/30 [03:10<12:37, 31.57s/it]Epoch 6\n",
      "Loss = 4.5524e-01, PNorm = 71.4459, GNorm = 0.6502, lr_0 = 7.0771e-04\n",
      "Loss = 4.6000e-01, PNorm = 71.5227, GNorm = 0.6158, lr_0 = 6.9593e-04\n",
      "Loss = 4.3979e-01, PNorm = 71.6080, GNorm = 0.8115, lr_0 = 6.8435e-04\n",
      "Loss = 4.7589e-01, PNorm = 71.6902, GNorm = 0.6316, lr_0 = 6.7296e-04\n",
      "Loss = 4.4829e-01, PNorm = 71.7757, GNorm = 0.6111, lr_0 = 6.6176e-04\n",
      "Validation auc = 0.791179\n",
      "Validation accuracy = 0.743548\n",
      " 23%|██▎       | 7/30 [03:41<12:05, 31.55s/it]Epoch 7\n",
      "Loss = 4.0394e-01, PNorm = 71.8703, GNorm = 1.3587, lr_0 = 6.4965e-04\n",
      "Loss = 4.3527e-01, PNorm = 71.9568, GNorm = 1.1780, lr_0 = 6.3884e-04\n",
      "Loss = 4.4697e-01, PNorm = 72.0393, GNorm = 0.6739, lr_0 = 6.2821e-04\n",
      "Loss = 4.5783e-01, PNorm = 72.1200, GNorm = 0.8326, lr_0 = 6.1776e-04\n",
      "Loss = 4.6045e-01, PNorm = 72.1939, GNorm = 0.8042, lr_0 = 6.0747e-04\n",
      "Validation auc = 0.821155\n",
      "Validation accuracy = 0.774194\n",
      " 27%|██▋       | 8/30 [04:14<11:46, 32.12s/it]Epoch 8\n",
      "Loss = 4.4751e-01, PNorm = 72.2664, GNorm = 0.6496, lr_0 = 5.9736e-04\n",
      "Loss = 3.9999e-01, PNorm = 72.3445, GNorm = 0.8934, lr_0 = 5.8742e-04\n",
      "Loss = 4.0214e-01, PNorm = 72.4147, GNorm = 1.0310, lr_0 = 5.7765e-04\n",
      "Loss = 4.3949e-01, PNorm = 72.4906, GNorm = 0.7564, lr_0 = 5.6803e-04\n",
      "Loss = 4.5458e-01, PNorm = 72.5765, GNorm = 1.1048, lr_0 = 5.5858e-04\n",
      "Validation auc = 0.802238\n",
      "Validation accuracy = 0.767742\n",
      " 30%|███       | 9/30 [04:47<11:14, 32.13s/it]Epoch 9\n",
      "Loss = 3.9475e-01, PNorm = 72.6582, GNorm = 0.9993, lr_0 = 5.4836e-04\n",
      "Loss = 3.5417e-01, PNorm = 72.7363, GNorm = 0.8926, lr_0 = 5.3924e-04\n",
      "Loss = 4.1715e-01, PNorm = 72.8049, GNorm = 1.1394, lr_0 = 5.3026e-04\n",
      "Loss = 4.1091e-01, PNorm = 72.8736, GNorm = 0.9302, lr_0 = 5.2144e-04\n",
      "Loss = 4.1942e-01, PNorm = 72.9547, GNorm = 0.7992, lr_0 = 5.1276e-04\n",
      "Validation auc = 0.792952\n",
      "Validation accuracy = 0.756452\n",
      " 33%|███▎      | 10/30 [05:18<10:38, 31.92s/it]Epoch 10\n",
      "Loss = 3.5256e-01, PNorm = 73.0328, GNorm = 0.8875, lr_0 = 5.0422e-04\n",
      "Loss = 4.0909e-01, PNorm = 73.1057, GNorm = 0.8817, lr_0 = 4.9583e-04\n",
      "Loss = 3.6912e-01, PNorm = 73.1761, GNorm = 1.3071, lr_0 = 4.8758e-04\n",
      "Loss = 3.8989e-01, PNorm = 73.2400, GNorm = 0.8724, lr_0 = 4.7947e-04\n",
      "Loss = 3.8619e-01, PNorm = 73.3114, GNorm = 1.6090, lr_0 = 4.7149e-04\n",
      "Validation auc = 0.788321\n",
      "Validation accuracy = 0.759677\n",
      " 37%|███▋      | 11/30 [05:51<10:15, 32.39s/it]Epoch 11\n",
      "Loss = 4.0163e-01, PNorm = 73.3928, GNorm = 0.9057, lr_0 = 4.6286e-04\n",
      "Loss = 3.3509e-01, PNorm = 73.4700, GNorm = 0.9940, lr_0 = 4.5516e-04\n",
      "Loss = 3.8721e-01, PNorm = 73.5360, GNorm = 1.1743, lr_0 = 4.4758e-04\n",
      "Loss = 3.6738e-01, PNorm = 73.6011, GNorm = 1.1657, lr_0 = 4.4014e-04\n",
      "Loss = 3.5329e-01, PNorm = 73.6704, GNorm = 1.2058, lr_0 = 4.3281e-04\n",
      "Validation auc = 0.820583\n",
      "Validation accuracy = 0.774194\n",
      " 40%|████      | 12/30 [06:23<09:38, 32.13s/it]Epoch 12\n",
      "Loss = 3.4315e-01, PNorm = 73.7332, GNorm = 0.9786, lr_0 = 4.2561e-04\n",
      "Loss = 3.3677e-01, PNorm = 73.8057, GNorm = 0.9763, lr_0 = 4.1852e-04\n",
      "Loss = 3.4181e-01, PNorm = 73.8767, GNorm = 1.1836, lr_0 = 4.1156e-04\n",
      "Loss = 3.2828e-01, PNorm = 73.9504, GNorm = 1.2222, lr_0 = 4.0471e-04\n",
      "Loss = 3.4057e-01, PNorm = 74.0192, GNorm = 1.7346, lr_0 = 3.9797e-04\n",
      "Validation auc = 0.815571\n",
      "Validation accuracy = 0.766129\n",
      " 43%|████▎     | 13/30 [06:53<08:53, 31.41s/it]Epoch 13\n",
      "Loss = 3.3832e-01, PNorm = 74.0917, GNorm = 1.1053, lr_0 = 3.9069e-04\n",
      "Loss = 3.4441e-01, PNorm = 74.1566, GNorm = 1.3069, lr_0 = 3.8419e-04\n",
      "Loss = 3.3815e-01, PNorm = 74.2146, GNorm = 1.1112, lr_0 = 3.7780e-04\n",
      "Loss = 3.3423e-01, PNorm = 74.2744, GNorm = 1.2224, lr_0 = 3.7151e-04\n",
      "Loss = 3.2401e-01, PNorm = 74.3335, GNorm = 0.7866, lr_0 = 3.6533e-04\n",
      "Validation auc = 0.787881\n",
      "Validation accuracy = 0.746774\n",
      " 47%|████▋     | 14/30 [07:23<08:14, 30.92s/it]Epoch 14\n",
      "Loss = 3.0000e-01, PNorm = 74.3945, GNorm = 0.9536, lr_0 = 3.5925e-04\n",
      "Loss = 2.8454e-01, PNorm = 74.4571, GNorm = 1.3739, lr_0 = 3.5327e-04\n",
      "Loss = 3.0929e-01, PNorm = 74.5240, GNorm = 1.2975, lr_0 = 3.4739e-04\n",
      "Loss = 3.4101e-01, PNorm = 74.5921, GNorm = 1.3897, lr_0 = 3.4161e-04\n",
      "Loss = 2.9423e-01, PNorm = 74.6525, GNorm = 1.2601, lr_0 = 3.3592e-04\n",
      "Validation auc = 0.810417\n",
      "Validation accuracy = 0.780645\n",
      " 50%|█████     | 15/30 [07:53<07:39, 30.66s/it]Epoch 15\n",
      "Loss = 2.5066e-01, PNorm = 74.7166, GNorm = 1.1206, lr_0 = 3.2978e-04\n",
      "Loss = 3.1702e-01, PNorm = 74.7754, GNorm = 1.5342, lr_0 = 3.2429e-04\n",
      "Loss = 3.0075e-01, PNorm = 74.8311, GNorm = 0.8361, lr_0 = 3.1889e-04\n",
      "Loss = 3.0472e-01, PNorm = 74.8918, GNorm = 1.4018, lr_0 = 3.1359e-04\n",
      "Loss = 2.9919e-01, PNorm = 74.9504, GNorm = 1.2366, lr_0 = 3.0837e-04\n",
      "Validation auc = 0.810202\n",
      "Validation accuracy = 0.741935\n",
      " 53%|█████▎    | 16/30 [08:23<07:06, 30.48s/it]Epoch 16\n",
      "Loss = 2.7687e-01, PNorm = 75.0133, GNorm = 1.2119, lr_0 = 3.0323e-04\n",
      "Loss = 3.1102e-01, PNorm = 75.0708, GNorm = 1.2454, lr_0 = 2.9819e-04\n",
      "Loss = 2.5266e-01, PNorm = 75.1253, GNorm = 1.1113, lr_0 = 2.9323e-04\n",
      "Loss = 2.8664e-01, PNorm = 75.1745, GNorm = 1.2099, lr_0 = 2.8835e-04\n",
      "Loss = 3.0279e-01, PNorm = 75.2165, GNorm = 1.3600, lr_0 = 2.8355e-04\n",
      "Validation auc = 0.802893\n",
      "Validation accuracy = 0.748387\n",
      " 57%|█████▋    | 17/30 [08:54<06:37, 30.61s/it]Epoch 17\n",
      "Loss = 2.5122e-01, PNorm = 75.2633, GNorm = 1.7281, lr_0 = 2.7836e-04\n",
      "Loss = 2.4578e-01, PNorm = 75.3096, GNorm = 1.2234, lr_0 = 2.7373e-04\n",
      "Loss = 2.6212e-01, PNorm = 75.3530, GNorm = 1.2090, lr_0 = 2.6917e-04\n",
      "Loss = 2.3909e-01, PNorm = 75.3941, GNorm = 1.3628, lr_0 = 2.6469e-04\n",
      "Loss = 2.7496e-01, PNorm = 75.4354, GNorm = 1.5833, lr_0 = 2.6029e-04\n",
      "Validation auc = 0.811095\n",
      "Validation accuracy = 0.782258\n",
      " 60%|██████    | 18/30 [09:25<06:08, 30.72s/it]Epoch 18\n",
      "Loss = 2.2917e-01, PNorm = 75.4778, GNorm = 1.5480, lr_0 = 2.5595e-04\n",
      "Loss = 2.6443e-01, PNorm = 75.5243, GNorm = 1.6245, lr_0 = 2.5170e-04\n",
      "Loss = 2.2194e-01, PNorm = 75.5722, GNorm = 1.4113, lr_0 = 2.4751e-04\n",
      "Loss = 2.8373e-01, PNorm = 75.6164, GNorm = 1.9608, lr_0 = 2.4339e-04\n",
      "Loss = 2.8082e-01, PNorm = 75.6559, GNorm = 1.5872, lr_0 = 2.3934e-04\n",
      "Loss = 2.7589e-01, PNorm = 75.6600, GNorm = 2.0937, lr_0 = 2.3894e-04\n",
      "Validation auc = 0.816560\n",
      "Validation accuracy = 0.769355\n",
      " 63%|██████▎   | 19/30 [09:57<05:44, 31.32s/it]Epoch 19\n",
      "Loss = 2.2403e-01, PNorm = 75.7057, GNorm = 2.0499, lr_0 = 2.3496e-04\n",
      "Loss = 2.2590e-01, PNorm = 75.7458, GNorm = 1.3780, lr_0 = 2.3105e-04\n",
      "Loss = 2.4395e-01, PNorm = 75.7898, GNorm = 1.4476, lr_0 = 2.2720e-04\n",
      "Loss = 2.1471e-01, PNorm = 75.8298, GNorm = 1.2405, lr_0 = 2.2342e-04\n",
      "Loss = 2.2754e-01, PNorm = 75.8738, GNorm = 2.6810, lr_0 = 2.1970e-04\n",
      "Validation auc = 0.814405\n",
      "Validation accuracy = 0.767742\n",
      " 67%|██████▋   | 20/30 [10:30<05:17, 31.70s/it]Epoch 20\n",
      "Loss = 1.8653e-01, PNorm = 75.9158, GNorm = 1.4432, lr_0 = 2.1605e-04\n",
      "Loss = 2.1756e-01, PNorm = 75.9477, GNorm = 1.4550, lr_0 = 2.1245e-04\n",
      "Loss = 2.3740e-01, PNorm = 75.9832, GNorm = 1.6800, lr_0 = 2.0892e-04\n",
      "Loss = 2.3584e-01, PNorm = 76.0178, GNorm = 1.2726, lr_0 = 2.0544e-04\n",
      "Validation auc = 0.802714\n",
      "Validation accuracy = 0.761290\n",
      " 70%|███████   | 21/30 [11:01<04:43, 31.50s/it]Epoch 21\n",
      "Loss = 1.9293e-01, PNorm = 76.0593, GNorm = 1.3819, lr_0 = 2.0168e-04\n",
      "Loss = 1.8748e-01, PNorm = 76.1006, GNorm = 1.4488, lr_0 = 1.9832e-04\n",
      "Loss = 1.8619e-01, PNorm = 76.1386, GNorm = 0.9574, lr_0 = 1.9502e-04\n",
      "Loss = 2.2710e-01, PNorm = 76.1720, GNorm = 2.0254, lr_0 = 1.9178e-04\n",
      "Loss = 1.9270e-01, PNorm = 76.2021, GNorm = 3.0987, lr_0 = 1.8859e-04\n",
      "Validation auc = 0.804095\n",
      "Validation accuracy = 0.761290\n",
      " 73%|███████▎  | 22/30 [11:33<04:13, 31.66s/it]Epoch 22\n",
      "Loss = 2.9433e-01, PNorm = 76.2295, GNorm = 1.7248, lr_0 = 1.8545e-04\n",
      "Loss = 1.5870e-01, PNorm = 76.2592, GNorm = 0.9275, lr_0 = 1.8236e-04\n",
      "Loss = 1.9868e-01, PNorm = 76.2898, GNorm = 1.4126, lr_0 = 1.7933e-04\n",
      "Loss = 1.9413e-01, PNorm = 76.3213, GNorm = 1.3137, lr_0 = 1.7634e-04\n",
      "Loss = 1.7653e-01, PNorm = 76.3514, GNorm = 1.9039, lr_0 = 1.7341e-04\n",
      "Validation auc = 0.805411\n",
      "Validation accuracy = 0.758065\n",
      " 77%|███████▋  | 23/30 [12:05<03:42, 31.84s/it]Epoch 23\n",
      "Loss = 1.9548e-01, PNorm = 76.3863, GNorm = 2.1735, lr_0 = 1.7024e-04\n",
      "Loss = 1.8030e-01, PNorm = 76.4168, GNorm = 1.0098, lr_0 = 1.6740e-04\n",
      "Loss = 1.9385e-01, PNorm = 76.4444, GNorm = 0.9900, lr_0 = 1.6462e-04\n",
      "Loss = 1.8308e-01, PNorm = 76.4694, GNorm = 1.5143, lr_0 = 1.6188e-04\n",
      "Loss = 1.8652e-01, PNorm = 76.4965, GNorm = 1.1875, lr_0 = 1.5918e-04\n",
      "Validation auc = 0.814452\n",
      "Validation accuracy = 0.764516\n",
      " 80%|████████  | 24/30 [12:41<03:19, 33.17s/it]Epoch 24\n",
      "Loss = 1.9093e-01, PNorm = 76.5211, GNorm = 1.3761, lr_0 = 1.5653e-04\n",
      "Loss = 1.3811e-01, PNorm = 76.5461, GNorm = 1.2804, lr_0 = 1.5393e-04\n",
      "Loss = 1.4923e-01, PNorm = 76.5723, GNorm = 1.4325, lr_0 = 1.5137e-04\n",
      "Loss = 2.1018e-01, PNorm = 76.5966, GNorm = 1.8369, lr_0 = 1.4885e-04\n",
      "Loss = 1.6379e-01, PNorm = 76.6213, GNorm = 1.3955, lr_0 = 1.4637e-04\n",
      "Validation auc = 0.799131\n",
      "Validation accuracy = 0.759677\n",
      " 83%|████████▎ | 25/30 [13:13<02:43, 32.62s/it]Epoch 25\n",
      "Loss = 1.4991e-01, PNorm = 76.6466, GNorm = 1.7056, lr_0 = 1.4393e-04\n",
      "Loss = 1.5626e-01, PNorm = 76.6715, GNorm = 1.2861, lr_0 = 1.4154e-04\n",
      "Loss = 1.3925e-01, PNorm = 76.6973, GNorm = 1.1728, lr_0 = 1.3918e-04\n",
      "Loss = 2.1251e-01, PNorm = 76.7165, GNorm = 1.8870, lr_0 = 1.3687e-04\n",
      "Loss = 1.5040e-01, PNorm = 76.7374, GNorm = 1.2770, lr_0 = 1.3459e-04\n",
      "Validation auc = 0.804768\n",
      "Validation accuracy = 0.759677\n",
      " 87%|████████▋ | 26/30 [13:44<02:08, 32.09s/it]Epoch 26\n",
      "Loss = 1.3228e-01, PNorm = 76.7587, GNorm = 1.4725, lr_0 = 1.3213e-04\n",
      "Loss = 1.5386e-01, PNorm = 76.7824, GNorm = 1.5530, lr_0 = 1.2993e-04\n",
      "Loss = 1.7390e-01, PNorm = 76.8041, GNorm = 1.3771, lr_0 = 1.2777e-04\n",
      "Loss = 1.3817e-01, PNorm = 76.8241, GNorm = 1.0521, lr_0 = 1.2564e-04\n",
      "Loss = 1.5283e-01, PNorm = 76.8441, GNorm = 1.4707, lr_0 = 1.2355e-04\n",
      "Validation auc = 0.804964\n",
      "Validation accuracy = 0.750000\n",
      " 90%|█████████ | 27/30 [14:16<01:36, 32.12s/it]Epoch 27\n",
      "Loss = 8.6697e-02, PNorm = 76.8664, GNorm = 0.8888, lr_0 = 1.2149e-04\n",
      "Loss = 1.1447e-01, PNorm = 76.8851, GNorm = 1.6731, lr_0 = 1.1947e-04\n",
      "Loss = 1.2256e-01, PNorm = 76.9050, GNorm = 1.4566, lr_0 = 1.1748e-04\n",
      "Loss = 1.5000e-01, PNorm = 76.9205, GNorm = 1.1007, lr_0 = 1.1553e-04\n",
      "Loss = 1.9582e-01, PNorm = 76.9345, GNorm = 1.9253, lr_0 = 1.1360e-04\n",
      "Validation auc = 0.808381\n",
      "Validation accuracy = 0.762903\n",
      " 93%|█████████▎| 28/30 [14:47<01:03, 31.80s/it]Epoch 28\n",
      "Loss = 1.4693e-01, PNorm = 76.9541, GNorm = 1.5230, lr_0 = 1.1153e-04\n",
      "Loss = 1.2568e-01, PNorm = 76.9704, GNorm = 2.2021, lr_0 = 1.0967e-04\n",
      "Loss = 1.5199e-01, PNorm = 76.9866, GNorm = 1.1944, lr_0 = 1.0784e-04\n",
      "Loss = 1.3147e-01, PNorm = 77.0009, GNorm = 1.1534, lr_0 = 1.0605e-04\n",
      "Loss = 1.6131e-01, PNorm = 77.0154, GNorm = 1.4277, lr_0 = 1.0428e-04\n",
      "Validation auc = 0.809435\n",
      "Validation accuracy = 0.761290\n",
      " 97%|█████████▋| 29/30 [15:18<00:31, 31.75s/it]Epoch 29\n",
      "Loss = 1.2540e-01, PNorm = 77.0313, GNorm = 1.5416, lr_0 = 1.0255e-04\n",
      "Loss = 1.1899e-01, PNorm = 77.0468, GNorm = 1.2235, lr_0 = 1.0084e-04\n",
      "Loss = 1.1039e-01, PNorm = 77.0632, GNorm = 1.6541, lr_0 = 1.0000e-04\n",
      "Loss = 1.5220e-01, PNorm = 77.0801, GNorm = 1.9294, lr_0 = 1.0000e-04\n",
      "Loss = 1.2451e-01, PNorm = 77.0945, GNorm = 1.3109, lr_0 = 1.0000e-04\n",
      "Validation auc = 0.800696\n",
      "Validation accuracy = 0.746774\n",
      "100%|██████████| 30/30 [15:49<00:00, 31.67s/it]\n",
      "Model 0 best validation auc = 0.821155 on epoch 7\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Loading pretrained parameter \"readout.7.weight\".\n",
      "Loading pretrained parameter \"readout.7.bias\".\n",
      "Model 0 test auc = 0.803285                    \n",
      "Model 0 test accuracy = 0.772903\n",
      "Building model 1\n",
      "MoleculeModel(\n",
      "  (sigmoid): Sigmoid()\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.15, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=1100, bias=False)\n",
      "        (W_h): Linear(in_features=1100, out_features=1100, bias=False)\n",
      "        (W_o): Linear(in_features=1233, out_features=1100, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.15, inplace=False)\n",
      "    (1): Linear(in_features=1300, out_features=1100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.15, inplace=False)\n",
      "    (4): Linear(in_features=1100, out_features=1100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.15, inplace=False)\n",
      "    (7): Linear(in_features=1100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 5,373,501\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Epoch 0\n",
      "Loss = 6.1745e-01, PNorm = 69.4137, GNorm = 2.0114, lr_0 = 2.0102e-04\n",
      "Loss = 6.1788e-01, PNorm = 69.4310, GNorm = 0.7992, lr_0 = 2.9286e-04\n",
      "Loss = 5.5693e-01, PNorm = 69.4579, GNorm = 1.6049, lr_0 = 3.8469e-04\n",
      "Loss = 6.1858e-01, PNorm = 69.4878, GNorm = 0.5782, lr_0 = 4.7653e-04\n",
      "Validation auc = 0.770095\n",
      "Validation accuracy = 0.711290\n",
      "  3%|▎         | 1/30 [00:35<16:55, 35.01s/it]Epoch 1\n",
      "Loss = 6.6061e-01, PNorm = 69.5360, GNorm = 0.9426, lr_0 = 5.7755e-04\n",
      "Loss = 5.7757e-01, PNorm = 69.5893, GNorm = 0.4096, lr_0 = 6.6939e-04\n",
      "Loss = 5.6568e-01, PNorm = 69.6589, GNorm = 0.5734, lr_0 = 7.6122e-04\n",
      "Loss = 5.3440e-01, PNorm = 69.7389, GNorm = 0.5406, lr_0 = 8.5306e-04\n",
      "Loss = 5.6099e-01, PNorm = 69.8257, GNorm = 0.8889, lr_0 = 9.4490e-04\n",
      "Validation auc = 0.790381\n",
      "Validation accuracy = 0.717742\n",
      "  7%|▋         | 2/30 [01:07<15:35, 33.41s/it]Epoch 2\n",
      "Loss = 5.1247e-01, PNorm = 69.9198, GNorm = 0.5572, lr_0 = 9.9331e-04\n",
      "Loss = 5.2321e-01, PNorm = 70.0184, GNorm = 0.4424, lr_0 = 9.7678e-04\n",
      "Loss = 5.5582e-01, PNorm = 70.0935, GNorm = 0.3929, lr_0 = 9.6052e-04\n",
      "Loss = 5.1321e-01, PNorm = 70.1707, GNorm = 0.5728, lr_0 = 9.4454e-04\n",
      "Loss = 5.1853e-01, PNorm = 70.2404, GNorm = 0.5605, lr_0 = 9.2882e-04\n",
      "Validation auc = 0.782393\n",
      "Validation accuracy = 0.753226\n",
      " 10%|█         | 3/30 [01:40<14:59, 33.30s/it]Epoch 3\n",
      "Loss = 5.6110e-01, PNorm = 70.3239, GNorm = 0.6402, lr_0 = 9.1183e-04\n",
      "Loss = 5.3607e-01, PNorm = 70.4082, GNorm = 1.0759, lr_0 = 8.9665e-04\n",
      "Loss = 5.7705e-01, PNorm = 70.4985, GNorm = 0.4405, lr_0 = 8.8173e-04\n",
      "Loss = 4.8512e-01, PNorm = 70.6062, GNorm = 0.6008, lr_0 = 8.6706e-04\n",
      "Loss = 5.1760e-01, PNorm = 70.6827, GNorm = 0.7165, lr_0 = 8.5262e-04\n",
      "Validation auc = 0.802988\n",
      "Validation accuracy = 0.766129\n",
      " 13%|█▎        | 4/30 [02:10<13:57, 32.20s/it]Epoch 4\n",
      "Loss = 4.9421e-01, PNorm = 70.7546, GNorm = 0.7981, lr_0 = 8.3843e-04\n",
      "Loss = 5.1448e-01, PNorm = 70.8227, GNorm = 0.4384, lr_0 = 8.2448e-04\n",
      "Loss = 4.9464e-01, PNorm = 70.8925, GNorm = 0.5436, lr_0 = 8.1076e-04\n",
      "Loss = 4.9907e-01, PNorm = 70.9705, GNorm = 0.9436, lr_0 = 7.9727e-04\n",
      "Loss = 4.4692e-01, PNorm = 71.0391, GNorm = 0.4997, lr_0 = 7.8400e-04\n",
      "Validation auc = 0.787810\n",
      "Validation accuracy = 0.745161\n",
      " 17%|█▋        | 5/30 [02:44<13:32, 32.51s/it]Epoch 5\n",
      "Loss = 4.1508e-01, PNorm = 71.1243, GNorm = 0.5315, lr_0 = 7.6966e-04\n",
      "Loss = 4.4242e-01, PNorm = 71.1910, GNorm = 0.6389, lr_0 = 7.5685e-04\n",
      "Loss = 4.7421e-01, PNorm = 71.2652, GNorm = 0.7420, lr_0 = 7.4425e-04\n",
      "Loss = 4.9109e-01, PNorm = 71.3420, GNorm = 0.9817, lr_0 = 7.3187e-04\n",
      "Loss = 5.3588e-01, PNorm = 71.4120, GNorm = 0.6337, lr_0 = 7.1969e-04\n",
      "Validation auc = 0.794821\n",
      "Validation accuracy = 0.740323\n",
      " 20%|██        | 6/30 [03:16<13:03, 32.65s/it]Epoch 6\n",
      "Loss = 4.7795e-01, PNorm = 71.4840, GNorm = 0.4902, lr_0 = 7.0771e-04\n",
      "Loss = 4.3014e-01, PNorm = 71.5516, GNorm = 0.5348, lr_0 = 6.9593e-04\n",
      "Loss = 4.9150e-01, PNorm = 71.6118, GNorm = 0.9605, lr_0 = 6.8435e-04\n",
      "Loss = 4.3586e-01, PNorm = 71.6806, GNorm = 0.9318, lr_0 = 6.7296e-04\n",
      "Loss = 4.7488e-01, PNorm = 71.7451, GNorm = 0.5394, lr_0 = 6.6176e-04\n",
      "Validation auc = 0.814786\n",
      "Validation accuracy = 0.761290\n",
      " 23%|██▎       | 7/30 [03:50<12:34, 32.80s/it]Epoch 7\n",
      "Loss = 4.7059e-01, PNorm = 71.8229, GNorm = 0.6543, lr_0 = 6.4965e-04\n",
      "Loss = 4.4616e-01, PNorm = 71.9048, GNorm = 0.6489, lr_0 = 6.3884e-04\n",
      "Loss = 4.6400e-01, PNorm = 71.9850, GNorm = 1.0783, lr_0 = 6.2821e-04\n",
      "Loss = 4.3002e-01, PNorm = 72.0630, GNorm = 0.7289, lr_0 = 6.1776e-04\n",
      "Loss = 4.3085e-01, PNorm = 72.1323, GNorm = 0.8479, lr_0 = 6.0747e-04\n",
      "Validation auc = 0.808131\n",
      "Validation accuracy = 0.748387\n",
      " 27%|██▋       | 8/30 [04:19<11:40, 31.86s/it]Epoch 8\n",
      "Loss = 4.1876e-01, PNorm = 72.1908, GNorm = 0.7391, lr_0 = 5.9736e-04\n",
      "Loss = 4.3891e-01, PNorm = 72.2503, GNorm = 0.7231, lr_0 = 5.8742e-04\n",
      "Loss = 4.3022e-01, PNorm = 72.3158, GNorm = 0.8034, lr_0 = 5.7765e-04\n",
      "Loss = 4.3741e-01, PNorm = 72.3839, GNorm = 1.2346, lr_0 = 5.6803e-04\n",
      "Loss = 3.9777e-01, PNorm = 72.4582, GNorm = 0.7944, lr_0 = 5.5858e-04\n",
      "Validation auc = 0.820821\n",
      "Validation accuracy = 0.762903\n",
      " 30%|███       | 9/30 [04:49<10:56, 31.26s/it]Epoch 9\n",
      "Loss = 4.4230e-01, PNorm = 72.5364, GNorm = 0.5925, lr_0 = 5.4836e-04\n",
      "Loss = 4.0278e-01, PNorm = 72.6018, GNorm = 1.2387, lr_0 = 5.3924e-04\n",
      "Loss = 4.0726e-01, PNorm = 72.6742, GNorm = 1.0426, lr_0 = 5.3026e-04\n",
      "Loss = 4.1734e-01, PNorm = 72.7335, GNorm = 0.7378, lr_0 = 5.2144e-04\n",
      "Loss = 4.4537e-01, PNorm = 72.8028, GNorm = 0.9938, lr_0 = 5.1276e-04\n",
      "Validation auc = 0.798619\n",
      "Validation accuracy = 0.750000\n",
      " 33%|███▎      | 10/30 [05:17<10:05, 30.29s/it]Epoch 10\n",
      "Loss = 3.5078e-01, PNorm = 72.8788, GNorm = 0.7358, lr_0 = 5.0422e-04\n",
      "Loss = 3.6514e-01, PNorm = 72.9559, GNorm = 0.7072, lr_0 = 4.9583e-04\n",
      "Loss = 4.0327e-01, PNorm = 73.0377, GNorm = 1.2259, lr_0 = 4.8758e-04\n",
      "Loss = 3.8921e-01, PNorm = 73.1089, GNorm = 1.0010, lr_0 = 4.7947e-04\n",
      "Loss = 4.3159e-01, PNorm = 73.1804, GNorm = 1.0578, lr_0 = 4.7149e-04\n",
      "Validation auc = 0.765786\n",
      "Validation accuracy = 0.743548\n",
      " 37%|███▋      | 11/30 [05:46<09:24, 29.69s/it]Epoch 11\n",
      "Loss = 3.7995e-01, PNorm = 73.2690, GNorm = 1.1093, lr_0 = 4.6286e-04\n",
      "Loss = 3.6912e-01, PNorm = 73.3465, GNorm = 0.6903, lr_0 = 4.5516e-04\n",
      "Loss = 3.6834e-01, PNorm = 73.4128, GNorm = 1.7847, lr_0 = 4.4758e-04\n",
      "Loss = 3.7411e-01, PNorm = 73.4794, GNorm = 1.0783, lr_0 = 4.4014e-04\n",
      "Loss = 3.9054e-01, PNorm = 73.5485, GNorm = 0.8410, lr_0 = 4.3281e-04\n",
      "Validation auc = 0.789060\n",
      "Validation accuracy = 0.764516\n",
      " 40%|████      | 12/30 [06:14<08:46, 29.25s/it]Epoch 12\n",
      "Loss = 3.6837e-01, PNorm = 73.6121, GNorm = 0.9364, lr_0 = 4.2561e-04\n",
      "Loss = 3.0876e-01, PNorm = 73.6855, GNorm = 0.8800, lr_0 = 4.1852e-04\n",
      "Loss = 3.7635e-01, PNorm = 73.7592, GNorm = 1.0234, lr_0 = 4.1156e-04\n",
      "Loss = 3.3237e-01, PNorm = 73.8296, GNorm = 1.0409, lr_0 = 4.0471e-04\n",
      "Loss = 4.0360e-01, PNorm = 73.8947, GNorm = 1.1360, lr_0 = 3.9797e-04\n",
      "Validation auc = 0.801000\n",
      "Validation accuracy = 0.740323\n",
      " 43%|████▎     | 13/30 [06:46<08:33, 30.22s/it]Epoch 13\n",
      "Loss = 3.0666e-01, PNorm = 73.9670, GNorm = 1.1331, lr_0 = 3.9069e-04\n",
      "Loss = 3.5668e-01, PNorm = 74.0409, GNorm = 1.1198, lr_0 = 3.8419e-04\n",
      "Loss = 3.4177e-01, PNorm = 74.1151, GNorm = 0.9775, lr_0 = 3.7780e-04\n",
      "Loss = 3.2025e-01, PNorm = 74.1872, GNorm = 1.3253, lr_0 = 3.7151e-04\n",
      "Loss = 3.1982e-01, PNorm = 74.2529, GNorm = 1.1666, lr_0 = 3.6533e-04\n",
      "Validation auc = 0.802619\n",
      "Validation accuracy = 0.764516\n",
      " 47%|████▋     | 14/30 [07:20<08:18, 31.14s/it]Epoch 14\n",
      "Loss = 2.9973e-01, PNorm = 74.3075, GNorm = 1.2252, lr_0 = 3.5925e-04\n",
      "Loss = 3.1200e-01, PNorm = 74.3711, GNorm = 0.9188, lr_0 = 3.5327e-04\n",
      "Loss = 2.9368e-01, PNorm = 74.4400, GNorm = 1.0896, lr_0 = 3.4739e-04\n",
      "Loss = 2.8652e-01, PNorm = 74.5073, GNorm = 1.1170, lr_0 = 3.4161e-04\n",
      "Loss = 3.0391e-01, PNorm = 74.5737, GNorm = 1.3281, lr_0 = 3.3592e-04\n",
      "Validation auc = 0.775119\n",
      "Validation accuracy = 0.745161\n",
      " 50%|█████     | 15/30 [07:52<07:50, 31.37s/it]Epoch 15\n",
      "Loss = 2.7890e-01, PNorm = 74.6359, GNorm = 1.1668, lr_0 = 3.2978e-04\n",
      "Loss = 3.0217e-01, PNorm = 74.6979, GNorm = 0.9180, lr_0 = 3.2429e-04\n",
      "Loss = 3.1071e-01, PNorm = 74.7578, GNorm = 1.4009, lr_0 = 3.1889e-04\n",
      "Loss = 2.9808e-01, PNorm = 74.8134, GNorm = 1.3556, lr_0 = 3.1359e-04\n",
      "Loss = 3.1665e-01, PNorm = 74.8665, GNorm = 1.0728, lr_0 = 3.0837e-04\n",
      "Validation auc = 0.786214\n",
      "Validation accuracy = 0.735484\n",
      " 53%|█████▎    | 16/30 [08:23<07:20, 31.50s/it]Epoch 16\n",
      "Loss = 2.7560e-01, PNorm = 74.9304, GNorm = 1.0604, lr_0 = 3.0323e-04\n",
      "Loss = 2.4944e-01, PNorm = 74.9974, GNorm = 1.3967, lr_0 = 2.9819e-04\n",
      "Loss = 2.6312e-01, PNorm = 75.0489, GNorm = 1.2675, lr_0 = 2.9323e-04\n",
      "Loss = 3.1699e-01, PNorm = 75.1024, GNorm = 1.7408, lr_0 = 2.8835e-04\n",
      "Loss = 2.9615e-01, PNorm = 75.1533, GNorm = 1.2547, lr_0 = 2.8355e-04\n",
      "Validation auc = 0.787190\n",
      "Validation accuracy = 0.759677\n",
      " 57%|█████▋    | 17/30 [08:54<06:47, 31.32s/it]Epoch 17\n",
      "Loss = 2.5636e-01, PNorm = 75.2050, GNorm = 1.4463, lr_0 = 2.7836e-04\n",
      "Loss = 2.6190e-01, PNorm = 75.2591, GNorm = 1.6388, lr_0 = 2.7373e-04\n",
      "Loss = 2.8601e-01, PNorm = 75.3036, GNorm = 1.3157, lr_0 = 2.6917e-04\n",
      "Loss = 2.8999e-01, PNorm = 75.3576, GNorm = 2.0600, lr_0 = 2.6469e-04\n",
      "Loss = 2.3142e-01, PNorm = 75.4086, GNorm = 1.2215, lr_0 = 2.6029e-04\n",
      "Validation auc = 0.805083\n",
      "Validation accuracy = 0.762903\n",
      " 60%|██████    | 18/30 [09:26<06:15, 31.33s/it]Epoch 18\n",
      "Loss = 2.4870e-01, PNorm = 75.4554, GNorm = 1.4639, lr_0 = 2.5595e-04\n",
      "Loss = 2.1110e-01, PNorm = 75.5011, GNorm = 1.0324, lr_0 = 2.5170e-04\n",
      "Loss = 2.3215e-01, PNorm = 75.5446, GNorm = 1.3285, lr_0 = 2.4751e-04\n",
      "Loss = 2.3481e-01, PNorm = 75.5854, GNorm = 0.9875, lr_0 = 2.4339e-04\n",
      "Loss = 2.5600e-01, PNorm = 75.6274, GNorm = 1.5125, lr_0 = 2.3934e-04\n",
      "Loss = 2.3038e-01, PNorm = 75.6321, GNorm = 2.2313, lr_0 = 2.3894e-04\n",
      "Validation auc = 0.809333\n",
      "Validation accuracy = 0.772581\n",
      " 63%|██████▎   | 19/30 [09:59<05:50, 31.87s/it]Epoch 19\n",
      "Loss = 2.0531e-01, PNorm = 75.6739, GNorm = 1.4529, lr_0 = 2.3496e-04\n",
      "Loss = 2.1747e-01, PNorm = 75.7188, GNorm = 0.9828, lr_0 = 2.3105e-04\n",
      "Loss = 2.6721e-01, PNorm = 75.7632, GNorm = 1.5723, lr_0 = 2.2720e-04\n",
      "Loss = 2.4618e-01, PNorm = 75.8051, GNorm = 1.2107, lr_0 = 2.2342e-04\n",
      "Loss = 2.4216e-01, PNorm = 75.8417, GNorm = 2.5257, lr_0 = 2.1970e-04\n",
      "Validation auc = 0.814143\n",
      "Validation accuracy = 0.756452\n",
      " 67%|██████▋   | 20/30 [10:28<05:11, 31.19s/it]Epoch 20\n",
      "Loss = 1.9793e-01, PNorm = 75.8853, GNorm = 1.1155, lr_0 = 2.1605e-04\n",
      "Loss = 2.4344e-01, PNorm = 75.9280, GNorm = 1.4899, lr_0 = 2.1245e-04\n",
      "Loss = 2.2565e-01, PNorm = 75.9691, GNorm = 1.7295, lr_0 = 2.0892e-04\n",
      "Loss = 2.1281e-01, PNorm = 76.0107, GNorm = 1.3411, lr_0 = 2.0544e-04\n",
      "Validation auc = 0.797833\n",
      "Validation accuracy = 0.756452\n",
      " 70%|███████   | 21/30 [10:59<04:37, 30.88s/it]Epoch 21\n",
      "Loss = 2.5090e-01, PNorm = 76.0543, GNorm = 1.6817, lr_0 = 2.0168e-04\n",
      "Loss = 2.2725e-01, PNorm = 76.0926, GNorm = 1.7475, lr_0 = 1.9832e-04\n",
      "Loss = 2.2149e-01, PNorm = 76.1251, GNorm = 1.6214, lr_0 = 1.9502e-04\n",
      "Loss = 2.0796e-01, PNorm = 76.1545, GNorm = 1.2578, lr_0 = 1.9178e-04\n",
      "Loss = 2.3024e-01, PNorm = 76.1854, GNorm = 1.5225, lr_0 = 1.8859e-04\n",
      "Validation auc = 0.797167\n",
      "Validation accuracy = 0.766129\n",
      " 73%|███████▎  | 22/30 [11:31<04:11, 31.47s/it]Epoch 22\n",
      "Loss = 1.2777e-01, PNorm = 76.2152, GNorm = 1.0888, lr_0 = 1.8545e-04\n",
      "Loss = 1.8179e-01, PNorm = 76.2492, GNorm = 1.2581, lr_0 = 1.8236e-04\n",
      "Loss = 2.2040e-01, PNorm = 76.2792, GNorm = 1.4260, lr_0 = 1.7933e-04\n",
      "Loss = 1.5455e-01, PNorm = 76.3092, GNorm = 1.3819, lr_0 = 1.7634e-04\n",
      "Loss = 1.9702e-01, PNorm = 76.3392, GNorm = 1.6964, lr_0 = 1.7341e-04\n",
      "Validation auc = 0.799929\n",
      "Validation accuracy = 0.770968\n",
      " 77%|███████▋  | 23/30 [12:04<03:43, 31.89s/it]Epoch 23\n",
      "Loss = 1.4370e-01, PNorm = 76.3678, GNorm = 1.1034, lr_0 = 1.7024e-04\n",
      "Loss = 1.9497e-01, PNorm = 76.4029, GNorm = 1.0451, lr_0 = 1.6740e-04\n",
      "Loss = 1.7145e-01, PNorm = 76.4319, GNorm = 1.5843, lr_0 = 1.6462e-04\n",
      "Loss = 1.9529e-01, PNorm = 76.4674, GNorm = 1.7238, lr_0 = 1.6188e-04\n",
      "Loss = 1.7898e-01, PNorm = 76.4997, GNorm = 2.1377, lr_0 = 1.5918e-04\n",
      "Validation auc = 0.806024\n",
      "Validation accuracy = 0.767742\n",
      " 80%|████████  | 24/30 [12:36<03:10, 31.76s/it]Epoch 24\n",
      "Loss = 1.8139e-01, PNorm = 76.5256, GNorm = 1.7201, lr_0 = 1.5653e-04\n",
      "Loss = 1.6643e-01, PNorm = 76.5535, GNorm = 1.7812, lr_0 = 1.5393e-04\n",
      "Loss = 1.7391e-01, PNorm = 76.5818, GNorm = 1.1143, lr_0 = 1.5137e-04\n",
      "Loss = 1.8042e-01, PNorm = 76.6089, GNorm = 1.7135, lr_0 = 1.4885e-04\n",
      "Loss = 1.7345e-01, PNorm = 76.6323, GNorm = 2.0132, lr_0 = 1.4637e-04\n",
      "Validation auc = 0.809161\n",
      "Validation accuracy = 0.764516\n",
      " 83%|████████▎ | 25/30 [13:09<02:41, 32.29s/it]Epoch 25\n",
      "Loss = 1.3895e-01, PNorm = 76.6531, GNorm = 1.3015, lr_0 = 1.4393e-04\n",
      "Loss = 1.5467e-01, PNorm = 76.6771, GNorm = 1.3044, lr_0 = 1.4154e-04\n",
      "Loss = 1.6516e-01, PNorm = 76.6962, GNorm = 2.5587, lr_0 = 1.3918e-04\n",
      "Loss = 1.5613e-01, PNorm = 76.7180, GNorm = 0.5373, lr_0 = 1.3687e-04\n",
      "Loss = 1.6990e-01, PNorm = 76.7384, GNorm = 1.9411, lr_0 = 1.3459e-04\n",
      "Validation auc = 0.797280\n",
      "Validation accuracy = 0.764516\n",
      " 87%|████████▋ | 26/30 [13:42<02:09, 32.39s/it]Epoch 26\n",
      "Loss = 1.4796e-01, PNorm = 76.7627, GNorm = 1.4425, lr_0 = 1.3213e-04\n",
      "Loss = 1.4480e-01, PNorm = 76.7856, GNorm = 1.5350, lr_0 = 1.2993e-04\n",
      "Loss = 1.4052e-01, PNorm = 76.8060, GNorm = 0.6517, lr_0 = 1.2777e-04\n",
      "Loss = 1.4999e-01, PNorm = 76.8254, GNorm = 1.2273, lr_0 = 1.2564e-04\n",
      "Loss = 1.8545e-01, PNorm = 76.8426, GNorm = 1.7180, lr_0 = 1.2355e-04\n",
      "Validation auc = 0.791720\n",
      "Validation accuracy = 0.766129\n",
      " 90%|█████████ | 27/30 [14:14<01:37, 32.44s/it]Epoch 27\n",
      "Loss = 9.6541e-02, PNorm = 76.8591, GNorm = 1.4029, lr_0 = 1.2149e-04\n",
      "Loss = 1.4699e-01, PNorm = 76.8780, GNorm = 1.7867, lr_0 = 1.1947e-04\n",
      "Loss = 1.6483e-01, PNorm = 76.8967, GNorm = 1.6396, lr_0 = 1.1748e-04\n",
      "Loss = 1.1673e-01, PNorm = 76.9137, GNorm = 1.1145, lr_0 = 1.1553e-04\n",
      "Loss = 1.4285e-01, PNorm = 76.9291, GNorm = 1.7947, lr_0 = 1.1360e-04\n",
      "Validation auc = 0.790149\n",
      "Validation accuracy = 0.759677\n",
      " 93%|█████████▎| 28/30 [14:45<01:03, 31.81s/it]Epoch 28\n",
      "Loss = 1.1682e-01, PNorm = 76.9476, GNorm = 1.3164, lr_0 = 1.1153e-04\n",
      "Loss = 1.4353e-01, PNorm = 76.9639, GNorm = 1.6768, lr_0 = 1.0967e-04\n",
      "Loss = 1.5570e-01, PNorm = 76.9813, GNorm = 1.7206, lr_0 = 1.0784e-04\n",
      "Loss = 1.3000e-01, PNorm = 76.9980, GNorm = 0.9703, lr_0 = 1.0605e-04\n",
      "Loss = 1.3745e-01, PNorm = 77.0146, GNorm = 1.1771, lr_0 = 1.0428e-04\n",
      "Validation auc = 0.802738\n",
      "Validation accuracy = 0.770968\n",
      " 97%|█████████▋| 29/30 [15:16<00:31, 31.51s/it]Epoch 29\n",
      "Loss = 1.0368e-01, PNorm = 77.0300, GNorm = 0.9927, lr_0 = 1.0255e-04\n",
      "Loss = 1.1017e-01, PNorm = 77.0443, GNorm = 1.5303, lr_0 = 1.0084e-04\n",
      "Loss = 1.3434e-01, PNorm = 77.0597, GNorm = 1.1084, lr_0 = 1.0000e-04\n",
      "Loss = 1.4106e-01, PNorm = 77.0745, GNorm = 1.6971, lr_0 = 1.0000e-04\n",
      "Loss = 1.3114e-01, PNorm = 77.0894, GNorm = 1.8552, lr_0 = 1.0000e-04\n",
      "Validation auc = 0.796393\n",
      "Validation accuracy = 0.770968\n",
      "100%|██████████| 30/30 [15:46<00:00, 31.55s/it]\n",
      "Model 1 best validation auc = 0.820821 on epoch 8\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Loading pretrained parameter \"readout.7.weight\".\n",
      "Loading pretrained parameter \"readout.7.bias\".\n",
      "Model 1 test auc = 0.807690                    \n",
      "Model 1 test accuracy = 0.762581\n",
      "Building model 2\n",
      "MoleculeModel(\n",
      "  (sigmoid): Sigmoid()\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.15, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=1100, bias=False)\n",
      "        (W_h): Linear(in_features=1100, out_features=1100, bias=False)\n",
      "        (W_o): Linear(in_features=1233, out_features=1100, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.15, inplace=False)\n",
      "    (1): Linear(in_features=1300, out_features=1100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.15, inplace=False)\n",
      "    (4): Linear(in_features=1100, out_features=1100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.15, inplace=False)\n",
      "    (7): Linear(in_features=1100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 5,373,501\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Epoch 0\n",
      "Loss = 6.4885e-01, PNorm = 69.4119, GNorm = 0.9076, lr_0 = 2.0102e-04\n",
      "Loss = 6.4095e-01, PNorm = 69.4296, GNorm = 1.8856, lr_0 = 2.9286e-04\n",
      "Loss = 5.8347e-01, PNorm = 69.4560, GNorm = 0.5463, lr_0 = 3.8469e-04\n",
      "Loss = 5.7403e-01, PNorm = 69.4875, GNorm = 0.5308, lr_0 = 4.7653e-04\n",
      "Validation auc = 0.768810\n",
      "Validation accuracy = 0.725806\n",
      "  3%|▎         | 1/30 [00:31<15:05, 31.21s/it]Epoch 1\n",
      "Loss = 5.8752e-01, PNorm = 69.5349, GNorm = 0.4924, lr_0 = 5.7755e-04\n",
      "Loss = 5.1742e-01, PNorm = 69.5956, GNorm = 1.0340, lr_0 = 6.6939e-04\n",
      "Loss = 5.7870e-01, PNorm = 69.6602, GNorm = 1.5295, lr_0 = 7.6122e-04\n",
      "Loss = 5.6740e-01, PNorm = 69.7532, GNorm = 0.4356, lr_0 = 8.5306e-04\n",
      "Loss = 6.1143e-01, PNorm = 69.8423, GNorm = 0.4055, lr_0 = 9.4490e-04\n",
      "Validation auc = 0.769238\n",
      "Validation accuracy = 0.737097\n",
      "  7%|▋         | 2/30 [01:02<14:35, 31.26s/it]Epoch 2\n",
      "Loss = 5.5228e-01, PNorm = 69.9339, GNorm = 0.4013, lr_0 = 9.9331e-04\n",
      "Loss = 5.6451e-01, PNorm = 70.0208, GNorm = 0.8839, lr_0 = 9.7678e-04\n",
      "Loss = 5.2185e-01, PNorm = 70.1143, GNorm = 0.6844, lr_0 = 9.6052e-04\n",
      "Loss = 5.4972e-01, PNorm = 70.1990, GNorm = 0.5929, lr_0 = 9.4454e-04\n",
      "Loss = 5.4075e-01, PNorm = 70.2766, GNorm = 0.4871, lr_0 = 9.2882e-04\n",
      "Validation auc = 0.795631\n",
      "Validation accuracy = 0.724194\n",
      " 10%|█         | 3/30 [01:33<13:55, 30.93s/it]Epoch 3\n",
      "Loss = 6.4566e-01, PNorm = 70.3639, GNorm = 0.7039, lr_0 = 9.1183e-04\n",
      "Loss = 5.4681e-01, PNorm = 70.4440, GNorm = 0.5331, lr_0 = 8.9665e-04\n",
      "Loss = 5.2312e-01, PNorm = 70.5117, GNorm = 0.4436, lr_0 = 8.8173e-04\n",
      "Loss = 4.7352e-01, PNorm = 70.5797, GNorm = 0.4330, lr_0 = 8.6706e-04\n",
      "Loss = 4.9860e-01, PNorm = 70.6430, GNorm = 0.4203, lr_0 = 8.5262e-04\n",
      "Validation auc = 0.779310\n",
      "Validation accuracy = 0.750000\n",
      " 13%|█▎        | 4/30 [02:02<13:11, 30.44s/it]Epoch 4\n",
      "Loss = 4.5689e-01, PNorm = 70.7138, GNorm = 0.5796, lr_0 = 8.3843e-04\n",
      "Loss = 5.0248e-01, PNorm = 70.7981, GNorm = 0.6159, lr_0 = 8.2448e-04\n",
      "Loss = 4.8445e-01, PNorm = 70.8851, GNorm = 0.7081, lr_0 = 8.1076e-04\n",
      "Loss = 5.3075e-01, PNorm = 70.9663, GNorm = 0.7584, lr_0 = 7.9727e-04\n",
      "Loss = 4.6934e-01, PNorm = 71.0512, GNorm = 0.6504, lr_0 = 7.8400e-04\n",
      "Validation auc = 0.791429\n",
      "Validation accuracy = 0.733871\n",
      " 17%|█▋        | 5/30 [02:31<12:27, 29.89s/it]Epoch 5\n",
      "Loss = 5.2500e-01, PNorm = 71.1423, GNorm = 0.8408, lr_0 = 7.6966e-04\n",
      "Loss = 5.0911e-01, PNorm = 71.2339, GNorm = 0.9527, lr_0 = 7.5685e-04\n",
      "Loss = 4.8030e-01, PNorm = 71.2958, GNorm = 0.4906, lr_0 = 7.4425e-04\n",
      "Loss = 4.6018e-01, PNorm = 71.3666, GNorm = 0.8163, lr_0 = 7.3187e-04\n",
      "Loss = 4.8681e-01, PNorm = 71.4321, GNorm = 0.7377, lr_0 = 7.1969e-04\n",
      "Validation auc = 0.804750\n",
      "Validation accuracy = 0.767742\n",
      " 20%|██        | 6/30 [03:00<11:49, 29.54s/it]Epoch 6\n",
      "Loss = 4.6491e-01, PNorm = 71.5014, GNorm = 1.2933, lr_0 = 7.0771e-04\n",
      "Loss = 4.5260e-01, PNorm = 71.5950, GNorm = 0.9117, lr_0 = 6.9593e-04\n",
      "Loss = 4.6897e-01, PNorm = 71.6746, GNorm = 0.7453, lr_0 = 6.8435e-04\n",
      "Loss = 4.2556e-01, PNorm = 71.7535, GNorm = 0.4963, lr_0 = 6.7296e-04\n",
      "Loss = 4.6450e-01, PNorm = 71.8169, GNorm = 0.6909, lr_0 = 6.6176e-04\n",
      "Validation auc = 0.818345\n",
      "Validation accuracy = 0.774194\n",
      " 23%|██▎       | 7/30 [03:31<11:31, 30.07s/it]Epoch 7\n",
      "Loss = 4.3778e-01, PNorm = 71.8894, GNorm = 0.8147, lr_0 = 6.4965e-04\n",
      "Loss = 4.4410e-01, PNorm = 71.9705, GNorm = 1.0039, lr_0 = 6.3884e-04\n",
      "Loss = 4.5397e-01, PNorm = 72.0566, GNorm = 1.0844, lr_0 = 6.2821e-04\n",
      "Loss = 4.6016e-01, PNorm = 72.1260, GNorm = 0.9449, lr_0 = 6.1776e-04\n",
      "Loss = 3.9137e-01, PNorm = 72.1927, GNorm = 0.7221, lr_0 = 6.0747e-04\n",
      "Validation auc = 0.811869\n",
      "Validation accuracy = 0.746774\n",
      " 27%|██▋       | 8/30 [04:02<11:07, 30.35s/it]Epoch 8\n",
      "Loss = 4.7259e-01, PNorm = 72.2670, GNorm = 0.6946, lr_0 = 5.9736e-04\n",
      "Loss = 4.4652e-01, PNorm = 72.3496, GNorm = 1.1720, lr_0 = 5.8742e-04\n",
      "Loss = 3.7899e-01, PNorm = 72.4312, GNorm = 0.9633, lr_0 = 5.7765e-04\n",
      "Loss = 4.3777e-01, PNorm = 72.5006, GNorm = 0.6339, lr_0 = 5.6803e-04\n",
      "Loss = 4.5997e-01, PNorm = 72.5711, GNorm = 0.6569, lr_0 = 5.5858e-04\n",
      "Validation auc = 0.813952\n",
      "Validation accuracy = 0.766129\n",
      " 30%|███       | 9/30 [04:33<10:40, 30.50s/it]Epoch 9\n",
      "Loss = 4.5167e-01, PNorm = 72.6472, GNorm = 0.8318, lr_0 = 5.4836e-04\n",
      "Loss = 3.5759e-01, PNorm = 72.7208, GNorm = 0.8313, lr_0 = 5.3924e-04\n",
      "Loss = 4.1890e-01, PNorm = 72.7894, GNorm = 1.0107, lr_0 = 5.3026e-04\n",
      "Loss = 3.7331e-01, PNorm = 72.8729, GNorm = 0.9449, lr_0 = 5.2144e-04\n",
      "Loss = 3.7269e-01, PNorm = 72.9543, GNorm = 0.8484, lr_0 = 5.1276e-04\n",
      "Validation auc = 0.805179\n",
      "Validation accuracy = 0.764516\n",
      " 33%|███▎      | 10/30 [05:04<10:13, 30.70s/it]Epoch 10\n",
      "Loss = 3.7839e-01, PNorm = 73.0370, GNorm = 0.9692, lr_0 = 5.0422e-04\n",
      "Loss = 3.7646e-01, PNorm = 73.1296, GNorm = 0.6767, lr_0 = 4.9583e-04\n",
      "Loss = 3.8433e-01, PNorm = 73.2014, GNorm = 0.5686, lr_0 = 4.8758e-04\n",
      "Loss = 4.0283e-01, PNorm = 73.2713, GNorm = 1.8830, lr_0 = 4.7947e-04\n",
      "Loss = 3.3161e-01, PNorm = 73.3403, GNorm = 1.0828, lr_0 = 4.7149e-04\n",
      "Validation auc = 0.807321\n",
      "Validation accuracy = 0.772581\n",
      " 37%|███▋      | 11/30 [05:39<10:08, 32.02s/it]Epoch 11\n",
      "Loss = 3.4153e-01, PNorm = 73.4147, GNorm = 0.9898, lr_0 = 4.6286e-04\n",
      "Loss = 3.5582e-01, PNorm = 73.4920, GNorm = 0.9983, lr_0 = 4.5516e-04\n",
      "Loss = 3.3397e-01, PNorm = 73.5664, GNorm = 0.9502, lr_0 = 4.4758e-04\n",
      "Loss = 3.8337e-01, PNorm = 73.6370, GNorm = 0.8525, lr_0 = 4.4014e-04\n",
      "Loss = 3.9353e-01, PNorm = 73.7070, GNorm = 0.8141, lr_0 = 4.3281e-04\n",
      "Validation auc = 0.798726\n",
      "Validation accuracy = 0.762903\n",
      " 40%|████      | 12/30 [06:10<09:28, 31.59s/it]Epoch 12\n",
      "Loss = 3.5022e-01, PNorm = 73.7801, GNorm = 0.7619, lr_0 = 4.2561e-04\n",
      "Loss = 3.5534e-01, PNorm = 73.8633, GNorm = 0.9766, lr_0 = 4.1852e-04\n",
      "Loss = 2.9905e-01, PNorm = 73.9485, GNorm = 1.8757, lr_0 = 4.1156e-04\n",
      "Loss = 3.4989e-01, PNorm = 74.0297, GNorm = 1.5134, lr_0 = 4.0471e-04\n",
      "Loss = 3.6475e-01, PNorm = 74.0968, GNorm = 1.3242, lr_0 = 3.9797e-04\n",
      "Validation auc = 0.813905\n",
      "Validation accuracy = 0.772581\n",
      " 43%|████▎     | 13/30 [06:40<08:48, 31.07s/it]Epoch 13\n",
      "Loss = 2.9697e-01, PNorm = 74.1681, GNorm = 1.0175, lr_0 = 3.9069e-04\n",
      "Loss = 3.0766e-01, PNorm = 74.2380, GNorm = 1.4594, lr_0 = 3.8419e-04\n",
      "Loss = 3.5585e-01, PNorm = 74.3080, GNorm = 1.4685, lr_0 = 3.7780e-04\n",
      "Loss = 3.4483e-01, PNorm = 74.3783, GNorm = 1.4384, lr_0 = 3.7151e-04\n",
      "Loss = 3.3141e-01, PNorm = 74.4415, GNorm = 0.9928, lr_0 = 3.6533e-04\n",
      "Validation auc = 0.806476\n",
      "Validation accuracy = 0.767742\n",
      " 47%|████▋     | 14/30 [07:10<08:11, 30.73s/it]Epoch 14\n",
      "Loss = 2.4620e-01, PNorm = 74.5019, GNorm = 0.7600, lr_0 = 3.5925e-04\n",
      "Loss = 2.8385e-01, PNorm = 74.5738, GNorm = 1.4200, lr_0 = 3.5327e-04\n",
      "Loss = 3.1831e-01, PNorm = 74.6330, GNorm = 0.9501, lr_0 = 3.4739e-04\n",
      "Loss = 3.3611e-01, PNorm = 74.6937, GNorm = 1.0529, lr_0 = 3.4161e-04\n",
      "Loss = 3.1843e-01, PNorm = 74.7542, GNorm = 1.1068, lr_0 = 3.3592e-04\n",
      "Validation auc = 0.814000\n",
      "Validation accuracy = 0.772581\n",
      " 50%|█████     | 15/30 [07:40<07:39, 30.65s/it]Epoch 15\n",
      "Loss = 3.1736e-01, PNorm = 74.8185, GNorm = 1.5441, lr_0 = 3.2978e-04\n",
      "Loss = 2.8191e-01, PNorm = 74.8819, GNorm = 1.6460, lr_0 = 3.2429e-04\n",
      "Loss = 2.7295e-01, PNorm = 74.9385, GNorm = 1.6769, lr_0 = 3.1889e-04\n",
      "Loss = 3.0105e-01, PNorm = 74.9863, GNorm = 1.3309, lr_0 = 3.1359e-04\n",
      "Loss = 2.5771e-01, PNorm = 75.0411, GNorm = 0.9161, lr_0 = 3.0837e-04\n",
      "Validation auc = 0.812786\n",
      "Validation accuracy = 0.775806\n",
      " 53%|█████▎    | 16/30 [08:11<07:09, 30.69s/it]Epoch 16\n",
      "Loss = 2.5875e-01, PNorm = 75.0977, GNorm = 1.1052, lr_0 = 3.0323e-04\n",
      "Loss = 2.1787e-01, PNorm = 75.1565, GNorm = 1.4297, lr_0 = 2.9819e-04\n",
      "Loss = 2.7682e-01, PNorm = 75.2104, GNorm = 1.2090, lr_0 = 2.9323e-04\n",
      "Loss = 3.1502e-01, PNorm = 75.2599, GNorm = 2.1361, lr_0 = 2.8835e-04\n",
      "Loss = 3.0682e-01, PNorm = 75.3096, GNorm = 1.8093, lr_0 = 2.8355e-04\n",
      "Validation auc = 0.794738\n",
      "Validation accuracy = 0.764516\n",
      " 57%|█████▋    | 17/30 [08:41<06:37, 30.54s/it]Epoch 17\n",
      "Loss = 2.3820e-01, PNorm = 75.3686, GNorm = 0.9073, lr_0 = 2.7836e-04\n",
      "Loss = 2.1005e-01, PNorm = 75.4200, GNorm = 0.9894, lr_0 = 2.7373e-04\n",
      "Loss = 2.6335e-01, PNorm = 75.4662, GNorm = 1.2809, lr_0 = 2.6917e-04\n",
      "Loss = 2.5073e-01, PNorm = 75.5115, GNorm = 1.1846, lr_0 = 2.6469e-04\n",
      "Loss = 2.7837e-01, PNorm = 75.5582, GNorm = 2.0318, lr_0 = 2.6029e-04\n",
      "Validation auc = 0.799012\n",
      "Validation accuracy = 0.779032\n",
      " 60%|██████    | 18/30 [09:11<06:05, 30.47s/it]Epoch 18\n",
      "Loss = 2.4527e-01, PNorm = 75.6037, GNorm = 1.3517, lr_0 = 2.5595e-04\n",
      "Loss = 2.5290e-01, PNorm = 75.6522, GNorm = 1.7224, lr_0 = 2.5170e-04\n",
      "Loss = 2.7304e-01, PNorm = 75.6939, GNorm = 0.8473, lr_0 = 2.4751e-04\n",
      "Loss = 2.5935e-01, PNorm = 75.7397, GNorm = 1.3865, lr_0 = 2.4339e-04\n",
      "Loss = 2.4168e-01, PNorm = 75.7824, GNorm = 1.3842, lr_0 = 2.3934e-04\n",
      "Loss = 1.4653e-01, PNorm = 75.7869, GNorm = 1.7004, lr_0 = 2.3894e-04\n",
      "Validation auc = 0.806405\n",
      "Validation accuracy = 0.770968\n",
      " 63%|██████▎   | 19/30 [09:43<05:39, 30.82s/it]Epoch 19\n",
      "Loss = 2.2569e-01, PNorm = 75.8365, GNorm = 1.8501, lr_0 = 2.3496e-04\n",
      "Loss = 2.0167e-01, PNorm = 75.8843, GNorm = 1.8752, lr_0 = 2.3105e-04\n",
      "Loss = 2.3474e-01, PNorm = 75.9222, GNorm = 1.8978, lr_0 = 2.2720e-04\n",
      "Loss = 1.9831e-01, PNorm = 75.9598, GNorm = 1.7230, lr_0 = 2.2342e-04\n",
      "Loss = 2.2998e-01, PNorm = 75.9984, GNorm = 2.0392, lr_0 = 2.1970e-04\n",
      "Validation auc = 0.800810\n",
      "Validation accuracy = 0.775806\n",
      " 67%|██████▋   | 20/30 [10:13<05:06, 30.64s/it]Epoch 20\n",
      "Loss = 2.3108e-01, PNorm = 76.0365, GNorm = 1.2195, lr_0 = 2.1605e-04\n",
      "Loss = 1.6698e-01, PNorm = 76.0745, GNorm = 1.3570, lr_0 = 2.1245e-04\n",
      "Loss = 2.1046e-01, PNorm = 76.1110, GNorm = 1.6212, lr_0 = 2.0892e-04\n",
      "Loss = 2.1323e-01, PNorm = 76.1498, GNorm = 1.6164, lr_0 = 2.0544e-04\n",
      "Validation auc = 0.796887\n",
      "Validation accuracy = 0.767742\n",
      " 70%|███████   | 21/30 [10:43<04:34, 30.54s/it]Epoch 21\n",
      "Loss = 2.2491e-01, PNorm = 76.1876, GNorm = 1.7065, lr_0 = 2.0168e-04\n",
      "Loss = 1.5397e-01, PNorm = 76.2218, GNorm = 1.1251, lr_0 = 1.9832e-04\n",
      "Loss = 2.0604e-01, PNorm = 76.2531, GNorm = 1.5288, lr_0 = 1.9502e-04\n",
      "Loss = 1.8823e-01, PNorm = 76.2856, GNorm = 1.1535, lr_0 = 1.9178e-04\n",
      "Loss = 2.2484e-01, PNorm = 76.3177, GNorm = 1.9066, lr_0 = 1.8859e-04\n",
      "Validation auc = 0.798048\n",
      "Validation accuracy = 0.748387\n",
      " 73%|███████▎  | 22/30 [11:15<04:06, 30.85s/it]Epoch 22\n",
      "Loss = 2.3663e-01, PNorm = 76.3475, GNorm = 1.3830, lr_0 = 1.8545e-04\n",
      "Loss = 1.5128e-01, PNorm = 76.3792, GNorm = 1.4838, lr_0 = 1.8236e-04\n",
      "Loss = 1.9174e-01, PNorm = 76.4091, GNorm = 1.8375, lr_0 = 1.7933e-04\n",
      "Loss = 1.5822e-01, PNorm = 76.4438, GNorm = 1.3339, lr_0 = 1.7634e-04\n",
      "Loss = 1.5625e-01, PNorm = 76.4760, GNorm = 1.3870, lr_0 = 1.7341e-04\n",
      "Validation auc = 0.802202\n",
      "Validation accuracy = 0.764516\n",
      " 77%|███████▋  | 23/30 [11:45<03:35, 30.72s/it]Epoch 23\n",
      "Loss = 2.0509e-01, PNorm = 76.5067, GNorm = 1.5012, lr_0 = 1.7024e-04\n",
      "Loss = 1.6403e-01, PNorm = 76.5325, GNorm = 1.6345, lr_0 = 1.6740e-04\n",
      "Loss = 1.6240e-01, PNorm = 76.5590, GNorm = 1.3522, lr_0 = 1.6462e-04\n",
      "Loss = 2.0372e-01, PNorm = 76.5837, GNorm = 2.1669, lr_0 = 1.6188e-04\n",
      "Loss = 1.9301e-01, PNorm = 76.6097, GNorm = 2.0379, lr_0 = 1.5918e-04\n",
      "Validation auc = 0.796667\n",
      "Validation accuracy = 0.767742\n",
      " 80%|████████  | 24/30 [12:17<03:06, 31.02s/it]Epoch 24\n",
      "Loss = 1.1123e-01, PNorm = 76.6413, GNorm = 0.8796, lr_0 = 1.5653e-04\n",
      "Loss = 1.3001e-01, PNorm = 76.6711, GNorm = 1.2240, lr_0 = 1.5393e-04\n",
      "Loss = 1.6732e-01, PNorm = 76.6968, GNorm = 2.1119, lr_0 = 1.5137e-04\n",
      "Loss = 1.9239e-01, PNorm = 76.7227, GNorm = 1.8857, lr_0 = 1.4885e-04\n",
      "Loss = 1.5102e-01, PNorm = 76.7479, GNorm = 1.2823, lr_0 = 1.4637e-04\n",
      "Validation auc = 0.802435\n",
      "Validation accuracy = 0.774194\n",
      " 83%|████████▎ | 25/30 [12:48<02:35, 31.04s/it]Epoch 25\n",
      "Loss = 1.6151e-01, PNorm = 76.7690, GNorm = 1.3523, lr_0 = 1.4393e-04\n",
      "Loss = 1.4346e-01, PNorm = 76.7900, GNorm = 1.2982, lr_0 = 1.4154e-04\n",
      "Loss = 1.4406e-01, PNorm = 76.8113, GNorm = 1.2087, lr_0 = 1.3918e-04\n",
      "Loss = 1.0957e-01, PNorm = 76.8348, GNorm = 1.0527, lr_0 = 1.3687e-04\n",
      "Loss = 1.3985e-01, PNorm = 76.8541, GNorm = 0.9822, lr_0 = 1.3459e-04\n",
      "Validation auc = 0.808530\n",
      "Validation accuracy = 0.767742\n",
      " 87%|████████▋ | 26/30 [13:22<02:07, 31.90s/it]Epoch 26\n",
      "Loss = 1.3291e-01, PNorm = 76.8782, GNorm = 1.4318, lr_0 = 1.3213e-04\n",
      "Loss = 1.5922e-01, PNorm = 76.8964, GNorm = 1.6628, lr_0 = 1.2993e-04\n",
      "Loss = 1.5116e-01, PNorm = 76.9177, GNorm = 1.7108, lr_0 = 1.2777e-04\n",
      "Loss = 1.3024e-01, PNorm = 76.9379, GNorm = 1.5474, lr_0 = 1.2564e-04\n",
      "Loss = 1.4698e-01, PNorm = 76.9568, GNorm = 1.1529, lr_0 = 1.2355e-04\n",
      "Validation auc = 0.798452\n",
      "Validation accuracy = 0.772581\n",
      " 90%|█████████ | 27/30 [13:53<01:34, 31.51s/it]Epoch 27\n",
      "Loss = 1.1920e-01, PNorm = 76.9751, GNorm = 1.6007, lr_0 = 1.2149e-04\n",
      "Loss = 1.2361e-01, PNorm = 76.9953, GNorm = 1.7478, lr_0 = 1.1947e-04\n",
      "Loss = 1.0137e-01, PNorm = 77.0147, GNorm = 1.3188, lr_0 = 1.1748e-04\n",
      "Loss = 1.1458e-01, PNorm = 77.0334, GNorm = 1.3564, lr_0 = 1.1553e-04\n",
      "Loss = 1.6616e-01, PNorm = 77.0493, GNorm = 2.3235, lr_0 = 1.1360e-04\n",
      "Validation auc = 0.787089\n",
      "Validation accuracy = 0.758065\n",
      " 93%|█████████▎| 28/30 [14:22<01:01, 30.84s/it]Epoch 28\n",
      "Loss = 1.0971e-01, PNorm = 77.0696, GNorm = 1.3258, lr_0 = 1.1153e-04\n",
      "Loss = 1.3510e-01, PNorm = 77.0846, GNorm = 1.9706, lr_0 = 1.0967e-04\n",
      "Loss = 1.4010e-01, PNorm = 77.1007, GNorm = 1.2467, lr_0 = 1.0784e-04\n",
      "Loss = 1.3296e-01, PNorm = 77.1181, GNorm = 1.2647, lr_0 = 1.0605e-04\n",
      "Loss = 1.4511e-01, PNorm = 77.1324, GNorm = 0.9327, lr_0 = 1.0428e-04\n",
      "Validation auc = 0.801923\n",
      "Validation accuracy = 0.770968\n",
      " 97%|█████████▋| 29/30 [14:51<00:30, 30.36s/it]Epoch 29\n",
      "Loss = 7.5862e-02, PNorm = 77.1454, GNorm = 0.8263, lr_0 = 1.0255e-04\n",
      "Loss = 1.4522e-01, PNorm = 77.1576, GNorm = 1.2027, lr_0 = 1.0084e-04\n",
      "Loss = 1.1414e-01, PNorm = 77.1725, GNorm = 0.5809, lr_0 = 1.0000e-04\n",
      "Loss = 1.4524e-01, PNorm = 77.1852, GNorm = 2.2015, lr_0 = 1.0000e-04\n",
      "Loss = 1.3644e-01, PNorm = 77.1986, GNorm = 1.9208, lr_0 = 1.0000e-04\n",
      "Validation auc = 0.787476\n",
      "Validation accuracy = 0.764516\n",
      "100%|██████████| 30/30 [15:21<00:00, 30.72s/it]\n",
      "Model 2 best validation auc = 0.818345 on epoch 6\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Loading pretrained parameter \"readout.7.weight\".\n",
      "Loading pretrained parameter \"readout.7.bias\".\n",
      "Model 2 test auc = 0.810755                    \n",
      "Model 2 test accuracy = 0.781935\n",
      "Building model 3\n",
      "MoleculeModel(\n",
      "  (sigmoid): Sigmoid()\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.15, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=1100, bias=False)\n",
      "        (W_h): Linear(in_features=1100, out_features=1100, bias=False)\n",
      "        (W_o): Linear(in_features=1233, out_features=1100, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.15, inplace=False)\n",
      "    (1): Linear(in_features=1300, out_features=1100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.15, inplace=False)\n",
      "    (4): Linear(in_features=1100, out_features=1100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.15, inplace=False)\n",
      "    (7): Linear(in_features=1100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 5,373,501\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Epoch 0\n",
      "Loss = 6.6870e-01, PNorm = 69.3915, GNorm = 1.8149, lr_0 = 2.0102e-04\n",
      "Loss = 6.3480e-01, PNorm = 69.4071, GNorm = 0.7050, lr_0 = 2.9286e-04\n",
      "Loss = 6.0351e-01, PNorm = 69.4332, GNorm = 0.6989, lr_0 = 3.8469e-04\n",
      "Loss = 5.8257e-01, PNorm = 69.4665, GNorm = 0.4609, lr_0 = 4.7653e-04\n",
      "Validation auc = 0.754655\n",
      "Validation accuracy = 0.750000\n",
      "  3%|▎         | 1/30 [00:31<14:59, 31.03s/it]Epoch 1\n",
      "Loss = 5.7543e-01, PNorm = 69.5093, GNorm = 0.7582, lr_0 = 5.7755e-04\n",
      "Loss = 5.4429e-01, PNorm = 69.5640, GNorm = 1.0158, lr_0 = 6.6939e-04\n",
      "Loss = 5.1771e-01, PNorm = 69.6289, GNorm = 0.5062, lr_0 = 7.6122e-04\n",
      "Loss = 5.9252e-01, PNorm = 69.7041, GNorm = 0.7390, lr_0 = 8.5306e-04\n",
      "Loss = 5.8129e-01, PNorm = 69.7985, GNorm = 0.5869, lr_0 = 9.4490e-04\n",
      "Validation auc = 0.775512\n",
      "Validation accuracy = 0.733871\n",
      "  7%|▋         | 2/30 [01:02<14:28, 31.04s/it]Epoch 2\n",
      "Loss = 5.6592e-01, PNorm = 69.9055, GNorm = 0.3934, lr_0 = 9.9331e-04\n",
      "Loss = 5.2313e-01, PNorm = 70.0224, GNorm = 0.8891, lr_0 = 9.7678e-04\n",
      "Loss = 5.8947e-01, PNorm = 70.1252, GNorm = 0.9227, lr_0 = 9.6052e-04\n",
      "Loss = 5.2748e-01, PNorm = 70.2315, GNorm = 0.7634, lr_0 = 9.4454e-04\n",
      "Loss = 5.5576e-01, PNorm = 70.3315, GNorm = 0.5624, lr_0 = 9.2882e-04\n",
      "Validation auc = 0.799643\n",
      "Validation accuracy = 0.751613\n",
      " 10%|█         | 3/30 [01:31<13:43, 30.49s/it]Epoch 3\n",
      "Loss = 5.3256e-01, PNorm = 70.4288, GNorm = 0.5090, lr_0 = 9.1183e-04\n",
      "Loss = 5.0990e-01, PNorm = 70.5172, GNorm = 0.6066, lr_0 = 8.9665e-04\n",
      "Loss = 5.1846e-01, PNorm = 70.6046, GNorm = 1.2029, lr_0 = 8.8173e-04\n",
      "Loss = 5.0854e-01, PNorm = 70.6759, GNorm = 0.6463, lr_0 = 8.6706e-04\n",
      "Loss = 5.2435e-01, PNorm = 70.7561, GNorm = 0.5201, lr_0 = 8.5262e-04\n",
      "Validation auc = 0.799679\n",
      "Validation accuracy = 0.750000\n",
      " 13%|█▎        | 4/30 [02:00<12:56, 29.87s/it]Epoch 4\n",
      "Loss = 3.5095e-01, PNorm = 70.8439, GNorm = 0.4849, lr_0 = 8.3843e-04\n",
      "Loss = 5.4164e-01, PNorm = 70.9357, GNorm = 1.1498, lr_0 = 8.2448e-04\n",
      "Loss = 4.8332e-01, PNorm = 71.0438, GNorm = 0.5108, lr_0 = 8.1076e-04\n",
      "Loss = 4.8498e-01, PNorm = 71.1328, GNorm = 0.5602, lr_0 = 7.9727e-04\n",
      "Loss = 5.0684e-01, PNorm = 71.2049, GNorm = 0.9536, lr_0 = 7.8400e-04\n",
      "Validation auc = 0.761738\n",
      "Validation accuracy = 0.724194\n",
      " 17%|█▋        | 5/30 [02:31<12:34, 30.20s/it]Epoch 5\n",
      "Loss = 4.2775e-01, PNorm = 71.2803, GNorm = 0.4936, lr_0 = 7.6966e-04\n",
      "Loss = 5.3484e-01, PNorm = 71.3514, GNorm = 0.4583, lr_0 = 7.5685e-04\n",
      "Loss = 4.4593e-01, PNorm = 71.4333, GNorm = 0.6950, lr_0 = 7.4425e-04\n",
      "Loss = 5.0675e-01, PNorm = 71.5063, GNorm = 0.7798, lr_0 = 7.3187e-04\n",
      "Loss = 4.5205e-01, PNorm = 71.5743, GNorm = 0.5008, lr_0 = 7.1969e-04\n",
      "Validation auc = 0.780786\n",
      "Validation accuracy = 0.748387\n",
      " 20%|██        | 6/30 [03:05<12:36, 31.52s/it]Epoch 6\n",
      "Loss = 4.6842e-01, PNorm = 71.6474, GNorm = 1.3170, lr_0 = 7.0771e-04\n",
      "Loss = 4.3199e-01, PNorm = 71.7183, GNorm = 0.8234, lr_0 = 6.9593e-04\n",
      "Loss = 4.4313e-01, PNorm = 71.7980, GNorm = 0.5326, lr_0 = 6.8435e-04\n",
      "Loss = 4.8879e-01, PNorm = 71.8703, GNorm = 0.7220, lr_0 = 6.7296e-04\n",
      "Loss = 4.5318e-01, PNorm = 71.9502, GNorm = 0.7324, lr_0 = 6.6176e-04\n",
      "Validation auc = 0.803833\n",
      "Validation accuracy = 0.762903\n",
      " 23%|██▎       | 7/30 [03:36<11:56, 31.16s/it]Epoch 7\n",
      "Loss = 3.6280e-01, PNorm = 72.0568, GNorm = 0.6926, lr_0 = 6.4965e-04\n",
      "Loss = 4.4778e-01, PNorm = 72.1447, GNorm = 0.7146, lr_0 = 6.3884e-04\n",
      "Loss = 4.3241e-01, PNorm = 72.2267, GNorm = 0.6743, lr_0 = 6.2821e-04\n",
      "Loss = 4.7534e-01, PNorm = 72.3142, GNorm = 1.1682, lr_0 = 6.1776e-04\n",
      "Loss = 4.3223e-01, PNorm = 72.3987, GNorm = 0.6763, lr_0 = 6.0747e-04\n",
      "Validation auc = 0.804571\n",
      "Validation accuracy = 0.762903\n",
      " 27%|██▋       | 8/30 [04:05<11:12, 30.58s/it]Epoch 8\n",
      "Loss = 3.6223e-01, PNorm = 72.4791, GNorm = 0.6768, lr_0 = 5.9736e-04\n",
      "Loss = 3.5754e-01, PNorm = 72.5707, GNorm = 0.6610, lr_0 = 5.8742e-04\n",
      "Loss = 4.1474e-01, PNorm = 72.6500, GNorm = 1.2010, lr_0 = 5.7765e-04\n",
      "Loss = 4.5912e-01, PNorm = 72.7265, GNorm = 0.9683, lr_0 = 5.6803e-04\n",
      "Loss = 4.2335e-01, PNorm = 72.8130, GNorm = 1.1448, lr_0 = 5.5858e-04\n",
      "Validation auc = 0.804393\n",
      "Validation accuracy = 0.751613\n",
      " 30%|███       | 9/30 [04:36<10:45, 30.72s/it]Epoch 9\n",
      "Loss = 4.1299e-01, PNorm = 72.9047, GNorm = 0.8519, lr_0 = 5.4836e-04\n",
      "Loss = 3.8671e-01, PNorm = 72.9815, GNorm = 0.8196, lr_0 = 5.3924e-04\n",
      "Loss = 4.3234e-01, PNorm = 73.0580, GNorm = 0.8763, lr_0 = 5.3026e-04\n",
      "Loss = 4.1464e-01, PNorm = 73.1407, GNorm = 0.9929, lr_0 = 5.2144e-04\n",
      "Loss = 3.5043e-01, PNorm = 73.2132, GNorm = 0.6572, lr_0 = 5.1276e-04\n",
      "Validation auc = 0.811881\n",
      "Validation accuracy = 0.766129\n",
      " 33%|███▎      | 10/30 [05:06<10:11, 30.60s/it]Epoch 10\n",
      "Loss = 3.6834e-01, PNorm = 73.2909, GNorm = 0.8291, lr_0 = 5.0422e-04\n",
      "Loss = 3.6869e-01, PNorm = 73.3801, GNorm = 0.9539, lr_0 = 4.9583e-04\n",
      "Loss = 3.8985e-01, PNorm = 73.4574, GNorm = 1.4461, lr_0 = 4.8758e-04\n",
      "Loss = 3.7164e-01, PNorm = 73.5452, GNorm = 0.8155, lr_0 = 4.7947e-04\n",
      "Loss = 3.9156e-01, PNorm = 73.6168, GNorm = 0.8742, lr_0 = 4.7149e-04\n",
      "Validation auc = 0.803810\n",
      "Validation accuracy = 0.767742\n",
      " 37%|███▋      | 11/30 [05:41<10:02, 31.73s/it]Epoch 11\n",
      "Loss = 3.3977e-01, PNorm = 73.7002, GNorm = 1.3433, lr_0 = 4.6286e-04\n",
      "Loss = 3.5975e-01, PNorm = 73.7882, GNorm = 0.7780, lr_0 = 4.5516e-04\n",
      "Loss = 3.4491e-01, PNorm = 73.8838, GNorm = 1.1364, lr_0 = 4.4758e-04\n",
      "Loss = 3.6544e-01, PNorm = 73.9705, GNorm = 0.9174, lr_0 = 4.4014e-04\n",
      "Loss = 4.1167e-01, PNorm = 74.0467, GNorm = 1.1136, lr_0 = 4.3281e-04\n",
      "Validation auc = 0.819155\n",
      "Validation accuracy = 0.767742\n",
      " 40%|████      | 12/30 [06:11<09:26, 31.48s/it]Epoch 12\n",
      "Loss = 3.3838e-01, PNorm = 74.1179, GNorm = 0.9671, lr_0 = 4.2561e-04\n",
      "Loss = 3.3746e-01, PNorm = 74.1922, GNorm = 0.9051, lr_0 = 4.1852e-04\n",
      "Loss = 3.2073e-01, PNorm = 74.2699, GNorm = 0.8312, lr_0 = 4.1156e-04\n",
      "Loss = 3.5252e-01, PNorm = 74.3445, GNorm = 1.0437, lr_0 = 4.0471e-04\n",
      "Loss = 3.5322e-01, PNorm = 74.4044, GNorm = 0.9956, lr_0 = 3.9797e-04\n",
      "Validation auc = 0.819333\n",
      "Validation accuracy = 0.775806\n",
      " 43%|████▎     | 13/30 [06:43<08:52, 31.34s/it]Epoch 13\n",
      "Loss = 3.1127e-01, PNorm = 74.4682, GNorm = 0.6896, lr_0 = 3.9069e-04\n",
      "Loss = 3.4969e-01, PNorm = 74.5327, GNorm = 0.8240, lr_0 = 3.8419e-04\n",
      "Loss = 3.1617e-01, PNorm = 74.6059, GNorm = 2.1780, lr_0 = 3.7780e-04\n",
      "Loss = 3.3784e-01, PNorm = 74.6692, GNorm = 0.9207, lr_0 = 3.7151e-04\n",
      "Loss = 2.9978e-01, PNorm = 74.7410, GNorm = 1.1848, lr_0 = 3.6533e-04\n",
      "Validation auc = 0.810262\n",
      "Validation accuracy = 0.767742\n",
      " 47%|████▋     | 14/30 [07:13<08:17, 31.08s/it]Epoch 14\n",
      "Loss = 2.8036e-01, PNorm = 74.8096, GNorm = 0.9678, lr_0 = 3.5925e-04\n",
      "Loss = 2.9557e-01, PNorm = 74.8838, GNorm = 1.6846, lr_0 = 3.5327e-04\n",
      "Loss = 2.9307e-01, PNorm = 74.9518, GNorm = 1.0597, lr_0 = 3.4739e-04\n",
      "Loss = 3.2822e-01, PNorm = 75.0125, GNorm = 0.9070, lr_0 = 3.4161e-04\n",
      "Loss = 3.3350e-01, PNorm = 75.0709, GNorm = 1.8115, lr_0 = 3.3592e-04\n",
      "Validation auc = 0.811929\n",
      "Validation accuracy = 0.772581\n",
      " 50%|█████     | 15/30 [07:44<07:47, 31.20s/it]Epoch 15\n",
      "Loss = 2.9494e-01, PNorm = 75.1351, GNorm = 1.0509, lr_0 = 3.2978e-04\n",
      "Loss = 2.9053e-01, PNorm = 75.1981, GNorm = 0.9656, lr_0 = 3.2429e-04\n",
      "Loss = 2.5980e-01, PNorm = 75.2718, GNorm = 0.8385, lr_0 = 3.1889e-04\n",
      "Loss = 2.8524e-01, PNorm = 75.3318, GNorm = 1.3818, lr_0 = 3.1359e-04\n",
      "Loss = 2.7927e-01, PNorm = 75.3860, GNorm = 1.7466, lr_0 = 3.0837e-04\n",
      "Validation auc = 0.798476\n",
      "Validation accuracy = 0.769355\n",
      " 53%|█████▎    | 16/30 [08:16<07:19, 31.37s/it]Epoch 16\n",
      "Loss = 2.7221e-01, PNorm = 75.4465, GNorm = 1.3972, lr_0 = 3.0323e-04\n",
      "Loss = 2.7998e-01, PNorm = 75.5126, GNorm = 1.6686, lr_0 = 2.9819e-04\n",
      "Loss = 2.5386e-01, PNorm = 75.5668, GNorm = 1.2480, lr_0 = 2.9323e-04\n",
      "Loss = 2.6331e-01, PNorm = 75.6203, GNorm = 0.9668, lr_0 = 2.8835e-04\n",
      "Loss = 2.7650e-01, PNorm = 75.6744, GNorm = 1.3772, lr_0 = 2.8355e-04\n",
      "Validation auc = 0.798512\n",
      "Validation accuracy = 0.770968\n",
      " 57%|█████▋    | 17/30 [08:47<06:43, 31.06s/it]Epoch 17\n",
      "Loss = 2.2687e-01, PNorm = 75.7300, GNorm = 1.0957, lr_0 = 2.7836e-04\n",
      "Loss = 2.6304e-01, PNorm = 75.7804, GNorm = 1.2950, lr_0 = 2.7373e-04\n",
      "Loss = 2.4249e-01, PNorm = 75.8344, GNorm = 0.8114, lr_0 = 2.6917e-04\n",
      "Loss = 2.5250e-01, PNorm = 75.8868, GNorm = 1.0702, lr_0 = 2.6469e-04\n",
      "Loss = 2.9008e-01, PNorm = 75.9343, GNorm = 1.2945, lr_0 = 2.6029e-04\n",
      "Validation auc = 0.814137\n",
      "Validation accuracy = 0.764516\n",
      " 60%|██████    | 18/30 [09:17<06:11, 30.97s/it]Epoch 18\n",
      "Loss = 2.4464e-01, PNorm = 75.9872, GNorm = 1.3042, lr_0 = 2.5595e-04\n",
      "Loss = 2.7843e-01, PNorm = 76.0390, GNorm = 1.3346, lr_0 = 2.5170e-04\n",
      "Loss = 2.2209e-01, PNorm = 76.0898, GNorm = 1.2650, lr_0 = 2.4751e-04\n",
      "Loss = 2.6162e-01, PNorm = 76.1367, GNorm = 1.2503, lr_0 = 2.4339e-04\n",
      "Loss = 2.4136e-01, PNorm = 76.1808, GNorm = 2.0447, lr_0 = 2.3934e-04\n",
      "Loss = 1.8973e-01, PNorm = 76.1847, GNorm = 1.5932, lr_0 = 2.3894e-04\n",
      "Validation auc = 0.816988\n",
      "Validation accuracy = 0.767742\n",
      " 63%|██████▎   | 19/30 [09:48<05:39, 30.89s/it]Epoch 19\n",
      "Loss = 1.9397e-01, PNorm = 76.2321, GNorm = 1.2120, lr_0 = 2.3496e-04\n",
      "Loss = 2.2813e-01, PNorm = 76.2768, GNorm = 1.2560, lr_0 = 2.3105e-04\n",
      "Loss = 2.3335e-01, PNorm = 76.3128, GNorm = 1.3079, lr_0 = 2.2720e-04\n",
      "Loss = 1.7780e-01, PNorm = 76.3464, GNorm = 1.0614, lr_0 = 2.2342e-04\n",
      "Loss = 2.6410e-01, PNorm = 76.3851, GNorm = 2.7679, lr_0 = 2.1970e-04\n",
      "Validation auc = 0.814637\n",
      "Validation accuracy = 0.766129\n",
      " 67%|██████▋   | 20/30 [10:20<05:13, 31.31s/it]Epoch 20\n",
      "Loss = 1.8902e-01, PNorm = 76.4308, GNorm = 0.9322, lr_0 = 2.1605e-04\n",
      "Loss = 2.2030e-01, PNorm = 76.4710, GNorm = 2.1647, lr_0 = 2.1245e-04\n",
      "Loss = 1.6302e-01, PNorm = 76.5052, GNorm = 1.2173, lr_0 = 2.0892e-04\n",
      "Loss = 1.9665e-01, PNorm = 76.5425, GNorm = 1.1647, lr_0 = 2.0544e-04\n",
      "Validation auc = 0.800411\n",
      "Validation accuracy = 0.772581\n",
      " 70%|███████   | 21/30 [10:51<04:39, 31.07s/it]Epoch 21\n",
      "Loss = 1.7547e-01, PNorm = 76.5831, GNorm = 1.1437, lr_0 = 2.0168e-04\n",
      "Loss = 1.9249e-01, PNorm = 76.6179, GNorm = 0.9629, lr_0 = 1.9832e-04\n",
      "Loss = 1.6907e-01, PNorm = 76.6582, GNorm = 1.5806, lr_0 = 1.9502e-04\n",
      "Loss = 1.8637e-01, PNorm = 76.6894, GNorm = 1.0909, lr_0 = 1.9178e-04\n",
      "Loss = 1.9170e-01, PNorm = 76.7246, GNorm = 1.4268, lr_0 = 1.8859e-04\n",
      "Validation auc = 0.797214\n",
      "Validation accuracy = 0.759677\n",
      " 73%|███████▎  | 22/30 [11:22<04:08, 31.03s/it]Epoch 22\n",
      "Loss = 2.8169e-01, PNorm = 76.7617, GNorm = 1.6157, lr_0 = 1.8545e-04\n",
      "Loss = 1.6774e-01, PNorm = 76.7978, GNorm = 1.9676, lr_0 = 1.8236e-04\n",
      "Loss = 1.6993e-01, PNorm = 76.8294, GNorm = 1.5816, lr_0 = 1.7933e-04\n",
      "Loss = 1.9805e-01, PNorm = 76.8610, GNorm = 1.3216, lr_0 = 1.7634e-04\n",
      "Loss = 1.8463e-01, PNorm = 76.8918, GNorm = 1.3118, lr_0 = 1.7341e-04\n",
      "Validation auc = 0.801839\n",
      "Validation accuracy = 0.766129\n",
      " 77%|███████▋  | 23/30 [11:53<03:36, 31.00s/it]Epoch 23\n",
      "Loss = 1.6295e-01, PNorm = 76.9249, GNorm = 1.2730, lr_0 = 1.7024e-04\n",
      "Loss = 1.6844e-01, PNorm = 76.9533, GNorm = 1.4360, lr_0 = 1.6740e-04\n",
      "Loss = 1.7593e-01, PNorm = 76.9828, GNorm = 1.0776, lr_0 = 1.6462e-04\n",
      "Loss = 1.8074e-01, PNorm = 77.0121, GNorm = 1.5989, lr_0 = 1.6188e-04\n",
      "Loss = 1.6878e-01, PNorm = 77.0390, GNorm = 1.0777, lr_0 = 1.5918e-04\n",
      "Validation auc = 0.803690\n",
      "Validation accuracy = 0.753226\n",
      " 80%|████████  | 24/30 [12:22<03:03, 30.62s/it]Epoch 24\n",
      "Loss = 1.3178e-01, PNorm = 77.0640, GNorm = 1.3529, lr_0 = 1.5653e-04\n",
      "Loss = 1.4646e-01, PNorm = 77.0876, GNorm = 1.7279, lr_0 = 1.5393e-04\n",
      "Loss = 1.1813e-01, PNorm = 77.1141, GNorm = 1.1833, lr_0 = 1.5137e-04\n",
      "Loss = 1.4728e-01, PNorm = 77.1409, GNorm = 1.5082, lr_0 = 1.4885e-04\n",
      "Loss = 1.6039e-01, PNorm = 77.1659, GNorm = 1.5181, lr_0 = 1.4637e-04\n",
      "Validation auc = 0.802000\n",
      "Validation accuracy = 0.767742\n",
      " 83%|████████▎ | 25/30 [12:58<02:40, 32.18s/it]Epoch 25\n",
      "Loss = 1.4929e-01, PNorm = 77.1901, GNorm = 1.4630, lr_0 = 1.4393e-04\n",
      "Loss = 1.2996e-01, PNorm = 77.2174, GNorm = 1.5106, lr_0 = 1.4154e-04\n",
      "Loss = 1.2113e-01, PNorm = 77.2413, GNorm = 1.1241, lr_0 = 1.3918e-04\n",
      "Loss = 1.5171e-01, PNorm = 77.2629, GNorm = 1.3980, lr_0 = 1.3687e-04\n",
      "Loss = 1.5915e-01, PNorm = 77.2856, GNorm = 1.2764, lr_0 = 1.3459e-04\n",
      "Validation auc = 0.794435\n",
      "Validation accuracy = 0.753226\n",
      " 87%|████████▋ | 26/30 [13:37<02:16, 34.21s/it]Epoch 26\n",
      "Loss = 1.1835e-01, PNorm = 77.3059, GNorm = 0.7857, lr_0 = 1.3213e-04\n",
      "Loss = 1.4685e-01, PNorm = 77.3266, GNorm = 1.0149, lr_0 = 1.2993e-04\n",
      "Loss = 1.5591e-01, PNorm = 77.3458, GNorm = 1.5432, lr_0 = 1.2777e-04\n",
      "Loss = 1.6772e-01, PNorm = 77.3668, GNorm = 1.3709, lr_0 = 1.2564e-04\n",
      "Loss = 1.4837e-01, PNorm = 77.3866, GNorm = 1.2418, lr_0 = 1.2355e-04\n",
      "Validation auc = 0.799048\n",
      "Validation accuracy = 0.762903\n",
      " 90%|█████████ | 27/30 [14:14<01:44, 34.91s/it]Epoch 27\n",
      "Loss = 1.4799e-01, PNorm = 77.4066, GNorm = 1.2011, lr_0 = 1.2149e-04\n",
      "Loss = 9.6971e-02, PNorm = 77.4268, GNorm = 0.9893, lr_0 = 1.1947e-04\n",
      "Loss = 1.3677e-01, PNorm = 77.4468, GNorm = 1.8858, lr_0 = 1.1748e-04\n",
      "Loss = 1.2340e-01, PNorm = 77.4628, GNorm = 0.9217, lr_0 = 1.1553e-04\n",
      "Loss = 1.4223e-01, PNorm = 77.4776, GNorm = 1.9797, lr_0 = 1.1360e-04\n",
      "Validation auc = 0.797869\n",
      "Validation accuracy = 0.770968\n",
      " 93%|█████████▎| 28/30 [14:50<01:10, 35.20s/it]Epoch 28\n",
      "Loss = 1.3009e-01, PNorm = 77.4935, GNorm = 1.7459, lr_0 = 1.1153e-04\n",
      "Loss = 1.2762e-01, PNorm = 77.5107, GNorm = 1.4440, lr_0 = 1.0967e-04\n",
      "Loss = 1.0058e-01, PNorm = 77.5279, GNorm = 1.8497, lr_0 = 1.0784e-04\n",
      "Loss = 9.9742e-02, PNorm = 77.5427, GNorm = 1.2953, lr_0 = 1.0605e-04\n",
      "Loss = 1.4130e-01, PNorm = 77.5590, GNorm = 1.5007, lr_0 = 1.0428e-04\n",
      "Validation auc = 0.799702\n",
      "Validation accuracy = 0.769355\n",
      " 97%|█████████▋| 29/30 [15:25<00:35, 35.20s/it]Epoch 29\n",
      "Loss = 1.0247e-01, PNorm = 77.5734, GNorm = 1.0626, lr_0 = 1.0255e-04\n",
      "Loss = 1.1159e-01, PNorm = 77.5881, GNorm = 0.9371, lr_0 = 1.0084e-04\n",
      "Loss = 9.5849e-02, PNorm = 77.6017, GNorm = 0.6485, lr_0 = 1.0000e-04\n",
      "Loss = 1.2965e-01, PNorm = 77.6150, GNorm = 1.4913, lr_0 = 1.0000e-04\n",
      "Loss = 1.1310e-01, PNorm = 77.6284, GNorm = 1.3205, lr_0 = 1.0000e-04\n",
      "Validation auc = 0.800375\n",
      "Validation accuracy = 0.769355\n",
      "100%|██████████| 30/30 [15:59<00:00, 31.97s/it]\n",
      "Model 3 best validation auc = 0.819333 on epoch 12\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Loading pretrained parameter \"readout.7.weight\".\n",
      "Loading pretrained parameter \"readout.7.bias\".\n",
      "Model 3 test auc = 0.816246                    \n",
      "Model 3 test accuracy = 0.776774\n",
      "Building model 4\n",
      "MoleculeModel(\n",
      "  (sigmoid): Sigmoid()\n",
      "  (encoder): MPN(\n",
      "    (encoder): ModuleList(\n",
      "      (0): MPNEncoder(\n",
      "        (dropout): Dropout(p=0.15, inplace=False)\n",
      "        (act_func): ReLU()\n",
      "        (W_i): Linear(in_features=147, out_features=1100, bias=False)\n",
      "        (W_h): Linear(in_features=1100, out_features=1100, bias=False)\n",
      "        (W_o): Linear(in_features=1233, out_features=1100, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (readout): Sequential(\n",
      "    (0): Dropout(p=0.15, inplace=False)\n",
      "    (1): Linear(in_features=1300, out_features=1100, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.15, inplace=False)\n",
      "    (4): Linear(in_features=1100, out_features=1100, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Dropout(p=0.15, inplace=False)\n",
      "    (7): Linear(in_features=1100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Number of parameters = 5,373,501\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Epoch 0\n",
      "Loss = 6.3832e-01, PNorm = 69.3814, GNorm = 0.8138, lr_0 = 2.0102e-04\n",
      "Loss = 6.2023e-01, PNorm = 69.3991, GNorm = 0.8962, lr_0 = 2.9286e-04\n",
      "Loss = 5.6739e-01, PNorm = 69.4253, GNorm = 0.6704, lr_0 = 3.8469e-04\n",
      "Loss = 6.1083e-01, PNorm = 69.4557, GNorm = 1.2598, lr_0 = 4.7653e-04\n",
      "Validation auc = 0.774262\n",
      "Validation accuracy = 0.745161\n",
      "  3%|▎         | 1/30 [00:34<16:45, 34.68s/it]Epoch 1\n",
      "Loss = 6.2110e-01, PNorm = 69.4969, GNorm = 0.8185, lr_0 = 5.7755e-04\n",
      "Loss = 5.6876e-01, PNorm = 69.5517, GNorm = 0.8855, lr_0 = 6.6939e-04\n",
      "Loss = 5.8652e-01, PNorm = 69.6173, GNorm = 0.9792, lr_0 = 7.6122e-04\n",
      "Loss = 5.6243e-01, PNorm = 69.6958, GNorm = 0.5274, lr_0 = 8.5306e-04\n",
      "Loss = 5.6391e-01, PNorm = 69.7843, GNorm = 0.6469, lr_0 = 9.4490e-04\n",
      "Validation auc = 0.782702\n",
      "Validation accuracy = 0.762903\n",
      "  7%|▋         | 2/30 [01:07<15:34, 33.39s/it]Epoch 2\n",
      "Loss = 4.4857e-01, PNorm = 69.8806, GNorm = 0.8834, lr_0 = 9.9331e-04\n",
      "Loss = 5.3280e-01, PNorm = 69.9837, GNorm = 0.7602, lr_0 = 9.7678e-04\n",
      "Loss = 5.4409e-01, PNorm = 70.0861, GNorm = 0.5976, lr_0 = 9.6052e-04\n",
      "Loss = 5.6725e-01, PNorm = 70.1883, GNorm = 0.4722, lr_0 = 9.4454e-04\n",
      "Loss = 5.2207e-01, PNorm = 70.2830, GNorm = 0.5782, lr_0 = 9.2882e-04\n",
      "Validation auc = 0.766988\n",
      "Validation accuracy = 0.727419\n",
      " 10%|█         | 3/30 [01:38<14:32, 32.30s/it]Epoch 3\n",
      "Loss = 4.9268e-01, PNorm = 70.3803, GNorm = 0.5652, lr_0 = 9.1183e-04\n",
      "Loss = 5.3393e-01, PNorm = 70.4746, GNorm = 0.7771, lr_0 = 8.9665e-04\n",
      "Loss = 5.1336e-01, PNorm = 70.5695, GNorm = 0.6090, lr_0 = 8.8173e-04\n",
      "Loss = 5.2686e-01, PNorm = 70.6628, GNorm = 0.4771, lr_0 = 8.6706e-04\n",
      "Loss = 5.0113e-01, PNorm = 70.7508, GNorm = 0.9908, lr_0 = 8.5262e-04\n",
      "Validation auc = 0.797857\n",
      "Validation accuracy = 0.753226\n",
      " 13%|█▎        | 4/30 [02:10<13:58, 32.24s/it]Epoch 4\n",
      "Loss = 4.7557e-01, PNorm = 70.8380, GNorm = 0.6424, lr_0 = 8.3843e-04\n",
      "Loss = 4.8562e-01, PNorm = 70.9381, GNorm = 1.2317, lr_0 = 8.2448e-04\n",
      "Loss = 5.0450e-01, PNorm = 71.0334, GNorm = 0.6103, lr_0 = 8.1076e-04\n",
      "Loss = 5.0976e-01, PNorm = 71.1315, GNorm = 0.8900, lr_0 = 7.9727e-04\n",
      "Loss = 4.5025e-01, PNorm = 71.2204, GNorm = 0.5697, lr_0 = 7.8400e-04\n",
      "Validation auc = 0.804690\n",
      "Validation accuracy = 0.722581\n",
      " 17%|█▋        | 5/30 [02:41<13:18, 31.94s/it]Epoch 5\n",
      "Loss = 5.5255e-01, PNorm = 71.3224, GNorm = 0.7655, lr_0 = 7.6966e-04\n",
      "Loss = 4.6088e-01, PNorm = 71.4125, GNorm = 0.5390, lr_0 = 7.5685e-04\n",
      "Loss = 4.4695e-01, PNorm = 71.4992, GNorm = 0.5490, lr_0 = 7.4425e-04\n",
      "Loss = 4.6035e-01, PNorm = 71.5966, GNorm = 0.7582, lr_0 = 7.3187e-04\n",
      "Loss = 4.8670e-01, PNorm = 71.6802, GNorm = 1.2656, lr_0 = 7.1969e-04\n",
      "Validation auc = 0.787702\n",
      "Validation accuracy = 0.751613\n",
      " 20%|██        | 6/30 [03:12<12:37, 31.58s/it]Epoch 6\n",
      "Loss = 5.1116e-01, PNorm = 71.7581, GNorm = 1.1517, lr_0 = 7.0771e-04\n",
      "Loss = 4.6517e-01, PNorm = 71.8426, GNorm = 0.7469, lr_0 = 6.9593e-04\n",
      "Loss = 4.4437e-01, PNorm = 71.9259, GNorm = 1.0234, lr_0 = 6.8435e-04\n",
      "Loss = 4.1758e-01, PNorm = 72.0076, GNorm = 0.8881, lr_0 = 6.7296e-04\n",
      "Loss = 4.7416e-01, PNorm = 72.0803, GNorm = 0.7496, lr_0 = 6.6176e-04\n",
      "Validation auc = 0.819881\n",
      "Validation accuracy = 0.738710\n",
      " 23%|██▎       | 7/30 [03:45<12:17, 32.06s/it]Epoch 7\n",
      "Loss = 3.8752e-01, PNorm = 72.1625, GNorm = 0.6440, lr_0 = 6.4965e-04\n",
      "Loss = 4.1617e-01, PNorm = 72.2382, GNorm = 0.7076, lr_0 = 6.3884e-04\n",
      "Loss = 4.5649e-01, PNorm = 72.3059, GNorm = 0.8609, lr_0 = 6.2821e-04\n",
      "Loss = 4.2917e-01, PNorm = 72.3779, GNorm = 0.9419, lr_0 = 6.1776e-04\n",
      "Loss = 4.1606e-01, PNorm = 72.4545, GNorm = 0.7941, lr_0 = 6.0747e-04\n",
      "Validation auc = 0.815798\n",
      "Validation accuracy = 0.754839\n",
      " 27%|██▋       | 8/30 [04:18<11:50, 32.31s/it]Epoch 8\n",
      "Loss = 3.2527e-01, PNorm = 72.5409, GNorm = 0.6494, lr_0 = 5.9736e-04\n",
      "Loss = 3.8405e-01, PNorm = 72.6303, GNorm = 0.8159, lr_0 = 5.8742e-04\n",
      "Loss = 4.5224e-01, PNorm = 72.7157, GNorm = 1.0607, lr_0 = 5.7765e-04\n",
      "Loss = 4.1994e-01, PNorm = 72.7897, GNorm = 0.9122, lr_0 = 5.6803e-04\n",
      "Loss = 4.2754e-01, PNorm = 72.8570, GNorm = 1.0867, lr_0 = 5.5858e-04\n",
      "Validation auc = 0.798821\n",
      "Validation accuracy = 0.761290\n",
      " 30%|███       | 9/30 [04:49<11:07, 31.80s/it]Epoch 9\n",
      "Loss = 3.9870e-01, PNorm = 72.9437, GNorm = 0.6600, lr_0 = 5.4836e-04\n",
      "Loss = 3.9248e-01, PNorm = 73.0257, GNorm = 0.7707, lr_0 = 5.3924e-04\n",
      "Loss = 4.3755e-01, PNorm = 73.0967, GNorm = 1.2284, lr_0 = 5.3026e-04\n",
      "Loss = 3.8351e-01, PNorm = 73.1678, GNorm = 0.7555, lr_0 = 5.2144e-04\n",
      "Loss = 4.0175e-01, PNorm = 73.2511, GNorm = 1.0201, lr_0 = 5.1276e-04\n",
      "Validation auc = 0.799845\n",
      "Validation accuracy = 0.741935\n",
      " 33%|███▎      | 10/30 [05:19<10:26, 31.34s/it]Epoch 10\n",
      "Loss = 3.3937e-01, PNorm = 73.3259, GNorm = 0.7702, lr_0 = 5.0422e-04\n",
      "Loss = 3.6872e-01, PNorm = 73.4110, GNorm = 1.2754, lr_0 = 4.9583e-04\n",
      "Loss = 3.4673e-01, PNorm = 73.4974, GNorm = 0.7987, lr_0 = 4.8758e-04\n",
      "Loss = 3.7837e-01, PNorm = 73.5596, GNorm = 0.9894, lr_0 = 4.7947e-04\n",
      "Loss = 4.0366e-01, PNorm = 73.6174, GNorm = 0.8046, lr_0 = 4.7149e-04\n",
      "Validation auc = 0.813845\n",
      "Validation accuracy = 0.764516\n",
      " 37%|███▋      | 11/30 [05:49<09:49, 31.01s/it]Epoch 11\n",
      "Loss = 3.1843e-01, PNorm = 73.6878, GNorm = 0.9839, lr_0 = 4.6286e-04\n",
      "Loss = 3.7926e-01, PNorm = 73.7608, GNorm = 1.3496, lr_0 = 4.5516e-04\n",
      "Loss = 3.4650e-01, PNorm = 73.8373, GNorm = 0.8723, lr_0 = 4.4758e-04\n",
      "Loss = 3.6430e-01, PNorm = 73.9091, GNorm = 1.2289, lr_0 = 4.4014e-04\n",
      "Loss = 3.5728e-01, PNorm = 73.9802, GNorm = 1.2698, lr_0 = 4.3281e-04\n",
      "Validation auc = 0.811298\n",
      "Validation accuracy = 0.761290\n",
      " 40%|████      | 12/30 [06:20<09:19, 31.07s/it]Epoch 12\n",
      "Loss = 3.0598e-01, PNorm = 74.0584, GNorm = 1.2231, lr_0 = 4.2561e-04\n",
      "Loss = 3.2180e-01, PNorm = 74.1361, GNorm = 0.8903, lr_0 = 4.1852e-04\n",
      "Loss = 3.7684e-01, PNorm = 74.2081, GNorm = 1.0139, lr_0 = 4.1156e-04\n",
      "Loss = 3.5591e-01, PNorm = 74.2776, GNorm = 0.9295, lr_0 = 4.0471e-04\n",
      "Loss = 3.4491e-01, PNorm = 74.3449, GNorm = 1.0227, lr_0 = 3.9797e-04\n",
      "Validation auc = 0.822964\n",
      "Validation accuracy = 0.785484\n",
      " 43%|████▎     | 13/30 [06:53<08:55, 31.51s/it]Epoch 13\n",
      "Loss = 2.8981e-01, PNorm = 74.4233, GNorm = 1.5024, lr_0 = 3.9069e-04\n",
      "Loss = 3.0407e-01, PNorm = 74.5003, GNorm = 1.1036, lr_0 = 3.8419e-04\n",
      "Loss = 3.1453e-01, PNorm = 74.5709, GNorm = 1.1098, lr_0 = 3.7780e-04\n",
      "Loss = 3.4088e-01, PNorm = 74.6402, GNorm = 1.2205, lr_0 = 3.7151e-04\n",
      "Loss = 3.2556e-01, PNorm = 74.7031, GNorm = 1.2325, lr_0 = 3.6533e-04\n",
      "Validation auc = 0.797869\n",
      "Validation accuracy = 0.761290\n",
      " 47%|████▋     | 14/30 [07:24<08:20, 31.26s/it]Epoch 14\n",
      "Loss = 2.9595e-01, PNorm = 74.7671, GNorm = 1.0045, lr_0 = 3.5925e-04\n",
      "Loss = 3.2241e-01, PNorm = 74.8379, GNorm = 1.1763, lr_0 = 3.5327e-04\n",
      "Loss = 2.8679e-01, PNorm = 74.9114, GNorm = 1.1116, lr_0 = 3.4739e-04\n",
      "Loss = 2.9660e-01, PNorm = 74.9756, GNorm = 1.0174, lr_0 = 3.4161e-04\n",
      "Loss = 3.1001e-01, PNorm = 75.0338, GNorm = 1.2811, lr_0 = 3.3592e-04\n",
      "Validation auc = 0.799786\n",
      "Validation accuracy = 0.764516\n",
      " 50%|█████     | 15/30 [08:10<08:58, 35.92s/it]Epoch 15\n",
      "Loss = 2.7162e-01, PNorm = 75.1034, GNorm = 0.9248, lr_0 = 3.2978e-04\n",
      "Loss = 2.8860e-01, PNorm = 75.1624, GNorm = 1.3714, lr_0 = 3.2429e-04\n",
      "Loss = 2.7591e-01, PNorm = 75.2199, GNorm = 1.1940, lr_0 = 3.1889e-04\n",
      "Loss = 2.8581e-01, PNorm = 75.2780, GNorm = 1.8969, lr_0 = 3.1359e-04\n",
      "Loss = 3.0290e-01, PNorm = 75.3260, GNorm = 1.6171, lr_0 = 3.0837e-04\n",
      "Validation auc = 0.806917\n",
      "Validation accuracy = 0.762903\n",
      " 53%|█████▎    | 16/30 [09:03<09:34, 41.05s/it]Epoch 16\n",
      "Loss = 2.4617e-01, PNorm = 75.3858, GNorm = 1.5089, lr_0 = 3.0323e-04\n",
      "Loss = 2.6590e-01, PNorm = 75.4422, GNorm = 1.4388, lr_0 = 2.9819e-04\n",
      "Loss = 2.5608e-01, PNorm = 75.4990, GNorm = 1.0883, lr_0 = 2.9323e-04\n",
      "Loss = 2.8956e-01, PNorm = 75.5424, GNorm = 1.5339, lr_0 = 2.8835e-04\n",
      "Loss = 2.5474e-01, PNorm = 75.5909, GNorm = 1.0910, lr_0 = 2.8355e-04\n",
      "Validation auc = 0.814524\n",
      "Validation accuracy = 0.774194\n",
      " 57%|█████▋    | 17/30 [09:55<09:36, 44.33s/it]Epoch 17\n",
      "Loss = 2.2583e-01, PNorm = 75.6501, GNorm = 1.1051, lr_0 = 2.7836e-04\n",
      "Loss = 2.2163e-01, PNorm = 75.7034, GNorm = 1.1768, lr_0 = 2.7373e-04\n",
      "Loss = 2.8064e-01, PNorm = 75.7533, GNorm = 1.4882, lr_0 = 2.6917e-04\n",
      "Loss = 2.7433e-01, PNorm = 75.8016, GNorm = 1.3158, lr_0 = 2.6469e-04\n",
      "Loss = 2.7328e-01, PNorm = 75.8504, GNorm = 1.4750, lr_0 = 2.6029e-04\n",
      "Validation auc = 0.814440\n",
      "Validation accuracy = 0.777419\n",
      " 60%|██████    | 18/30 [10:28<08:09, 40.82s/it]Epoch 18\n",
      "Loss = 2.1572e-01, PNorm = 75.9005, GNorm = 1.1115, lr_0 = 2.5595e-04\n",
      "Loss = 2.3537e-01, PNorm = 75.9509, GNorm = 1.0356, lr_0 = 2.5170e-04\n",
      "Loss = 2.3086e-01, PNorm = 76.0009, GNorm = 1.7086, lr_0 = 2.4751e-04\n",
      "Loss = 2.2150e-01, PNorm = 76.0469, GNorm = 1.6134, lr_0 = 2.4339e-04\n",
      "Loss = 2.4270e-01, PNorm = 76.0889, GNorm = 1.7783, lr_0 = 2.3934e-04\n",
      "Loss = 1.1706e-01, PNorm = 76.0932, GNorm = 1.3751, lr_0 = 2.3894e-04\n",
      "Validation auc = 0.814452\n",
      "Validation accuracy = 0.780645\n",
      " 63%|██████▎   | 19/30 [11:00<06:58, 38.06s/it]Epoch 19\n",
      "Loss = 2.2682e-01, PNorm = 76.1414, GNorm = 2.3505, lr_0 = 2.3496e-04\n",
      "Loss = 1.9344e-01, PNorm = 76.1906, GNorm = 1.5138, lr_0 = 2.3105e-04\n",
      "Loss = 2.1386e-01, PNorm = 76.2344, GNorm = 1.3136, lr_0 = 2.2720e-04\n",
      "Loss = 1.9401e-01, PNorm = 76.2749, GNorm = 1.6310, lr_0 = 2.2342e-04\n",
      "Loss = 2.3669e-01, PNorm = 76.3125, GNorm = 2.4127, lr_0 = 2.1970e-04\n",
      "Validation auc = 0.792506\n",
      "Validation accuracy = 0.758065\n",
      " 67%|██████▋   | 20/30 [11:27<05:48, 34.85s/it]Epoch 20\n",
      "Loss = 1.8185e-01, PNorm = 76.3548, GNorm = 1.1009, lr_0 = 2.1605e-04\n",
      "Loss = 2.0476e-01, PNorm = 76.3961, GNorm = 1.5815, lr_0 = 2.1245e-04\n",
      "Loss = 1.9094e-01, PNorm = 76.4351, GNorm = 1.2717, lr_0 = 2.0892e-04\n",
      "Loss = 1.8668e-01, PNorm = 76.4756, GNorm = 1.9093, lr_0 = 2.0544e-04\n",
      "Validation auc = 0.806863\n",
      "Validation accuracy = 0.766129\n",
      " 70%|███████   | 21/30 [11:53<04:51, 32.34s/it]Epoch 21\n",
      "Loss = 1.6575e-01, PNorm = 76.5170, GNorm = 1.0049, lr_0 = 2.0168e-04\n",
      "Loss = 1.8090e-01, PNorm = 76.5574, GNorm = 1.9746, lr_0 = 1.9832e-04\n",
      "Loss = 1.7921e-01, PNorm = 76.5947, GNorm = 1.0909, lr_0 = 1.9502e-04\n",
      "Loss = 1.7672e-01, PNorm = 76.6313, GNorm = 0.9551, lr_0 = 1.9178e-04\n",
      "Loss = 1.5210e-01, PNorm = 76.6674, GNorm = 0.9377, lr_0 = 1.8859e-04\n",
      "Validation auc = 0.771887\n",
      "Validation accuracy = 0.738710\n",
      " 73%|███████▎  | 22/30 [12:21<04:07, 30.90s/it]Epoch 22\n",
      "Loss = 1.6032e-01, PNorm = 76.7039, GNorm = 1.2656, lr_0 = 1.8545e-04\n",
      "Loss = 1.6806e-01, PNorm = 76.7403, GNorm = 1.6267, lr_0 = 1.8236e-04\n",
      "Loss = 1.9006e-01, PNorm = 76.7759, GNorm = 1.3969, lr_0 = 1.7933e-04\n",
      "Loss = 1.9245e-01, PNorm = 76.8035, GNorm = 2.1356, lr_0 = 1.7634e-04\n",
      "Loss = 1.8006e-01, PNorm = 76.8339, GNorm = 1.4485, lr_0 = 1.7341e-04\n",
      "Validation auc = 0.804042\n",
      "Validation accuracy = 0.762903\n",
      " 77%|███████▋  | 23/30 [12:48<03:28, 29.84s/it]Epoch 23\n",
      "Loss = 1.7068e-01, PNorm = 76.8642, GNorm = 1.5786, lr_0 = 1.7024e-04\n",
      "Loss = 1.5801e-01, PNorm = 76.8914, GNorm = 1.3298, lr_0 = 1.6740e-04\n",
      "Loss = 1.4593e-01, PNorm = 76.9212, GNorm = 1.0451, lr_0 = 1.6462e-04\n",
      "Loss = 1.7251e-01, PNorm = 76.9507, GNorm = 1.9403, lr_0 = 1.6188e-04\n",
      "Loss = 1.8781e-01, PNorm = 76.9810, GNorm = 2.0475, lr_0 = 1.5918e-04\n",
      "Validation auc = 0.804000\n",
      "Validation accuracy = 0.766129\n",
      " 80%|████████  | 24/30 [13:18<02:57, 29.64s/it]Epoch 24\n",
      "Loss = 1.5930e-01, PNorm = 77.0093, GNorm = 1.1906, lr_0 = 1.5653e-04\n",
      "Loss = 1.2732e-01, PNorm = 77.0406, GNorm = 0.9635, lr_0 = 1.5393e-04\n",
      "Loss = 1.2423e-01, PNorm = 77.0686, GNorm = 1.1909, lr_0 = 1.5137e-04\n",
      "Loss = 1.5164e-01, PNorm = 77.0957, GNorm = 1.8539, lr_0 = 1.4885e-04\n",
      "Loss = 1.2792e-01, PNorm = 77.1183, GNorm = 1.4349, lr_0 = 1.4637e-04\n",
      "Validation auc = 0.799768\n",
      "Validation accuracy = 0.754839\n",
      " 83%|████████▎ | 25/30 [13:44<02:23, 28.61s/it]Epoch 25\n",
      "Loss = 1.1946e-01, PNorm = 77.1434, GNorm = 1.2742, lr_0 = 1.4393e-04\n",
      "Loss = 1.3827e-01, PNorm = 77.1711, GNorm = 1.6855, lr_0 = 1.4154e-04\n",
      "Loss = 1.5963e-01, PNorm = 77.1933, GNorm = 1.6522, lr_0 = 1.3918e-04\n",
      "Loss = 1.5378e-01, PNorm = 77.2142, GNorm = 2.5913, lr_0 = 1.3687e-04\n",
      "Loss = 1.3115e-01, PNorm = 77.2325, GNorm = 1.1678, lr_0 = 1.3459e-04\n",
      "Validation auc = 0.801137\n",
      "Validation accuracy = 0.766129\n",
      " 87%|████████▋ | 26/30 [14:10<01:51, 27.79s/it]Epoch 26\n",
      "Loss = 9.0945e-02, PNorm = 77.2558, GNorm = 1.3847, lr_0 = 1.3213e-04\n",
      "Loss = 1.3050e-01, PNorm = 77.2735, GNorm = 0.6192, lr_0 = 1.2993e-04\n",
      "Loss = 1.3083e-01, PNorm = 77.2956, GNorm = 1.3265, lr_0 = 1.2777e-04\n",
      "Loss = 1.4212e-01, PNorm = 77.3181, GNorm = 1.0036, lr_0 = 1.2564e-04\n",
      "Loss = 1.5381e-01, PNorm = 77.3364, GNorm = 1.4930, lr_0 = 1.2355e-04\n",
      "Validation auc = 0.813976\n",
      "Validation accuracy = 0.777419\n",
      " 90%|█████████ | 27/30 [14:37<01:22, 27.56s/it]Epoch 27\n",
      "Loss = 1.1705e-01, PNorm = 77.3540, GNorm = 1.6481, lr_0 = 1.2149e-04\n",
      "Loss = 1.2397e-01, PNorm = 77.3718, GNorm = 1.9700, lr_0 = 1.1947e-04\n",
      "Loss = 1.4069e-01, PNorm = 77.3880, GNorm = 1.1046, lr_0 = 1.1748e-04\n",
      "Loss = 1.1199e-01, PNorm = 77.4054, GNorm = 1.4250, lr_0 = 1.1553e-04\n",
      "Loss = 1.3964e-01, PNorm = 77.4209, GNorm = 2.0925, lr_0 = 1.1360e-04\n",
      "Validation auc = 0.809798\n",
      "Validation accuracy = 0.769355\n",
      " 93%|█████████▎| 28/30 [15:03<00:54, 27.35s/it]Epoch 28\n",
      "Loss = 8.3398e-02, PNorm = 77.4391, GNorm = 1.3825, lr_0 = 1.1153e-04\n",
      "Loss = 1.1024e-01, PNorm = 77.4541, GNorm = 1.6000, lr_0 = 1.0967e-04\n",
      "Loss = 1.1316e-01, PNorm = 77.4707, GNorm = 0.5637, lr_0 = 1.0784e-04\n",
      "Loss = 1.1581e-01, PNorm = 77.4853, GNorm = 1.3703, lr_0 = 1.0605e-04\n",
      "Loss = 1.3560e-01, PNorm = 77.4997, GNorm = 1.9528, lr_0 = 1.0428e-04\n",
      "Validation auc = 0.804560\n",
      "Validation accuracy = 0.762903\n",
      " 97%|█████████▋| 29/30 [15:32<00:27, 27.67s/it]Epoch 29\n",
      "Loss = 6.9643e-02, PNorm = 77.5146, GNorm = 1.1226, lr_0 = 1.0255e-04\n",
      "Loss = 1.1316e-01, PNorm = 77.5290, GNorm = 1.1448, lr_0 = 1.0084e-04\n",
      "Loss = 1.0092e-01, PNorm = 77.5433, GNorm = 0.9165, lr_0 = 1.0000e-04\n",
      "Loss = 9.6148e-02, PNorm = 77.5566, GNorm = 0.9330, lr_0 = 1.0000e-04\n",
      "Loss = 1.1108e-01, PNorm = 77.5706, GNorm = 1.5396, lr_0 = 1.0000e-04\n",
      "Validation auc = 0.804071\n",
      "Validation accuracy = 0.777419\n",
      "100%|██████████| 30/30 [16:02<00:00, 32.10s/it]\n",
      "Model 4 best validation auc = 0.822964 on epoch 12\n",
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Loading pretrained parameter \"readout.7.weight\".\n",
      "Loading pretrained parameter \"readout.7.bias\".\n",
      "Model 4 test auc = 0.823030                    \n",
      "Model 4 test accuracy = 0.774194\n",
      "Ensemble test auc = 0.820558\n",
      "Ensemble test accuracy = 0.785806\n",
      "1-fold cross validation\n",
      "\tSeed 0 ==> test auc = 0.820558\n",
      "\tSeed 0 ==> test accuracy = 0.785806\n",
      "Overall test auc = 0.820558 +/- 0.000000\n",
      "Overall test accuracy = 0.785806 +/- 0.000000\n",
      "Elapsed time = 1:23:31\n"
     ]
    }
   ],
   "source": [
    "train_filename = 'train.csv'\n",
    "val_filename = 'val.csv'\n",
    "arguments = [\n",
    "    '--data_path', os.path.join(endpoint_loc, train_filename),\n",
    "    '--dataset_type', 'classification',\n",
    "    '--config_path', os.path.join(model_loc, 'config.json'),\n",
    "    '--separate_test_path', os.path.join(endpoint_loc, val_filename), \n",
    "    '--save_dir', model_loc,\n",
    "    '--target_columns', 'Activity', \n",
    "    '--smiles_columns', 'SMILES',\n",
    "    '--features_generator', 'rdkit_2d_normalized', \n",
    "    '--no_features_scaling', \n",
    "    '--ensemble_size', '5', \n",
    "    '--extra_metrics', 'accuracy'\n",
    "]\n",
    "\n",
    "args = chemprop.args.TrainArgs().parse_args(arguments)\n",
    "# chemprop.train.run_training(args=args)\n",
    "mean_score, std_score = chemprop.train.cross_validate(args=args, train_func=chemprop.train.run_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training args\n",
      "Setting molecule featurization parameters to default.\n",
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "776it [00:00, 129239.99it/s]\n",
      "100%|██████████| 776/776 [00:45<00:00, 17.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating SMILES\n",
      "Test size = 775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Loading pretrained parameter \"readout.7.weight\".\n",
      "Loading pretrained parameter \"readout.7.bias\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:57<03:48, 57.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Loading pretrained parameter \"readout.7.weight\".\n",
      "Loading pretrained parameter \"readout.7.bias\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [01:38<02:23, 47.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Loading pretrained parameter \"readout.7.weight\".\n",
      "Loading pretrained parameter \"readout.7.bias\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [02:18<01:28, 44.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Loading pretrained parameter \"readout.7.weight\".\n",
      "Loading pretrained parameter \"readout.7.bias\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [02:57<00:42, 42.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained parameter \"encoder.encoder.0.cached_zero_vector\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_i.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_h.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.weight\".\n",
      "Loading pretrained parameter \"encoder.encoder.0.W_o.bias\".\n",
      "Loading pretrained parameter \"readout.1.weight\".\n",
      "Loading pretrained parameter \"readout.1.bias\".\n",
      "Loading pretrained parameter \"readout.4.weight\".\n",
      "Loading pretrained parameter \"readout.4.bias\".\n",
      "Loading pretrained parameter \"readout.7.weight\".\n",
      "Loading pretrained parameter \"readout.7.bias\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:38<00:00, 43.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving predictions to /dev/null\n",
      "Elapsed time = 0:04:24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_filename = 'val.csv'\n",
    "arguments = [\n",
    "    '--test_path', os.path.join(endpoint_loc, val_filename), \n",
    "    '--preds_path', '/dev/null',\n",
    "    '--checkpoint_dir', model_loc,\n",
    "    '--smiles_columns', 'SMILES',\n",
    "    '--features_generator', 'rdkit_2d_normalized', \n",
    "    '--no_features_scaling'\n",
    "]\n",
    "\n",
    "args = chemprop.args.PredictArgs().parse_args(arguments)\n",
    "preds = chemprop.train.make_predictions(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5945665657520294],\n",
       " [0.7223759055137634],\n",
       " [0.5087614178657531],\n",
       " [0.5976842999458313],\n",
       " [0.8043069005012512],\n",
       " [0.5421594381332397],\n",
       " [0.16182066947221757],\n",
       " [0.9534872770309448],\n",
       " [0.9069293737411499],\n",
       " [0.902154004573822],\n",
       " [0.1844962164759636],\n",
       " [0.37136169373989103],\n",
       " [0.9182733058929443],\n",
       " [0.8462374567985534],\n",
       " [0.34891530573368074],\n",
       " [0.8048105478286743],\n",
       " [0.9527966499328613],\n",
       " [0.6063166081905365],\n",
       " [0.12867302745580672],\n",
       " [0.9477950692176819],\n",
       " [0.920025908946991],\n",
       " [0.5724234938621521],\n",
       " [0.956162965297699],\n",
       " [0.45280042886734007],\n",
       " [0.9971102476119995],\n",
       " [0.8108995199203491],\n",
       " [0.6743675947189331],\n",
       " [0.4976506412029266],\n",
       " [0.3179577499628067],\n",
       " [0.23984797298908234],\n",
       " [0.9318285346031189],\n",
       " [0.4140376508235931],\n",
       " [0.8964445948600769],\n",
       " [0.8131973147392273],\n",
       " [0.6404035329818726],\n",
       " [0.8279972076416016],\n",
       " [0.6614622831344604],\n",
       " [0.7690639138221741],\n",
       " [0.9885819435119629],\n",
       " [0.9709149599075317],\n",
       " [0.5818681120872498],\n",
       " [0.23676041662693023],\n",
       " [0.016925808042287827],\n",
       " [0.729566776752472],\n",
       " [0.4284004867076874],\n",
       " [0.15240045934915541],\n",
       " [0.7044929146766663],\n",
       " [0.7450776815414428],\n",
       " [0.6388954639434814],\n",
       " [0.6270207285881042],\n",
       " [0.38819649815559387],\n",
       " [0.07545851022005082],\n",
       " [0.7510257720947265],\n",
       " [0.9303119301795959],\n",
       " [0.4383817851543427],\n",
       " [0.8589231014251709],\n",
       " [0.7567942857742309],\n",
       " [0.9554527640342713],\n",
       " [0.7808120012283325],\n",
       " [0.3970841646194458],\n",
       " [0.8856273293495178],\n",
       " [0.4104229211807251],\n",
       " [0.9014675855636597],\n",
       " [0.45164108872413633],\n",
       " [0.34713452160358427],\n",
       " [0.424470990896225],\n",
       " [0.8007638931274415],\n",
       " [0.958551573753357],\n",
       " [0.9194647669792175],\n",
       " [0.8077767491340637],\n",
       " [0.8019021987915039],\n",
       " [0.940321433544159],\n",
       " [0.20677289217710496],\n",
       " [0.6698394894599915],\n",
       " [0.7700910329818725],\n",
       " [0.7202293753623963],\n",
       " [0.4101473569869995],\n",
       " [0.26690585911273956],\n",
       " [0.9302292346954346],\n",
       " [0.8173514485359192],\n",
       " [0.8391196370124817],\n",
       " [0.5729719519615173],\n",
       " [0.4654863715171814],\n",
       " [0.6655099272727967],\n",
       " [0.4294646382331848],\n",
       " [0.017133686039596797],\n",
       " [0.6001527190208436],\n",
       " [0.8889698386192322],\n",
       " [0.9382956624031067],\n",
       " [0.6694547891616821],\n",
       " [0.5293987572193146],\n",
       " [0.18161985278129578],\n",
       " [0.32419400215148925],\n",
       " [0.9595130085945129],\n",
       " [0.7157864928245544],\n",
       " [0.9949254989624023],\n",
       " [0.41566080451011655],\n",
       " [0.3817142605781555],\n",
       " [0.3837648630142212],\n",
       " [0.930777370929718],\n",
       " [0.994939661026001],\n",
       " [0.9386744260787964],\n",
       " [0.5874496698379517],\n",
       " [0.7206796526908874],\n",
       " [0.9916501045227051],\n",
       " [0.7021813988685608],\n",
       " [0.46073039174079894],\n",
       " [0.9658198356628418],\n",
       " [0.6912890553474427],\n",
       " [0.5323122322559357],\n",
       " [0.4209472477436066],\n",
       " [0.9206193327903748],\n",
       " [0.8057774782180787],\n",
       " [0.39927724599838255],\n",
       " [0.9884443402290344],\n",
       " [0.6720622777938843],\n",
       " [0.9473248362541199],\n",
       " [0.9956095457077027],\n",
       " [0.04325268603861332],\n",
       " [0.9229494452476501],\n",
       " [0.4914009034633636],\n",
       " [0.8442951679229737],\n",
       " [0.9961951613426209],\n",
       " [0.3060410887002945],\n",
       " [0.9733357906341553],\n",
       " [0.5892644107341767],\n",
       " [0.32064348757266997],\n",
       " [0.8452005386352539],\n",
       " [0.8558750987052918],\n",
       " [0.6008814036846161],\n",
       " [0.9794444441795349],\n",
       " [0.8046868562698364],\n",
       " [0.7924319505691528],\n",
       " [0.7719776868820191],\n",
       " [0.8777769327163696],\n",
       " [0.9246072053909302],\n",
       " [0.026396140269935132],\n",
       " [0.34108140170574186],\n",
       " [0.9816664934158326],\n",
       " [0.92450110912323],\n",
       " [0.8032189130783081],\n",
       " [0.8805976390838623],\n",
       " [0.9977978348731995],\n",
       " [0.4238492727279663],\n",
       " [0.726511013507843],\n",
       " [0.47232113480567933],\n",
       " [0.11249197423458099],\n",
       " [0.6344764113426209],\n",
       " [0.7076822876930237],\n",
       " [0.894350266456604],\n",
       " [0.21363297551870347],\n",
       " [0.8861767530441285],\n",
       " [0.7162082076072693],\n",
       " [0.5382797837257385],\n",
       " [0.7956038117408752],\n",
       " [0.5446731448173523],\n",
       " [0.9640053987503052],\n",
       " [0.2873418629169464],\n",
       " [0.15355246961116792],\n",
       " [0.9652172923088074],\n",
       " [0.39877171218395235],\n",
       " [0.3820463389158249],\n",
       " [0.8613705277442932],\n",
       " [0.4992185473442078],\n",
       " [0.8322684168815613],\n",
       " [0.5318863213062286],\n",
       " [0.9812178373336792],\n",
       " [0.662362027168274],\n",
       " [0.5714544892311096],\n",
       " [0.9814458370208741],\n",
       " [0.9450479388237],\n",
       " [0.4833288826048374],\n",
       " [0.7033166408538818],\n",
       " [0.8052920699119568],\n",
       " [0.9179431676864624],\n",
       " [0.9144040584564209],\n",
       " [0.5787552118301391],\n",
       " [0.9489790201187134],\n",
       " [0.5049242556095124],\n",
       " [0.9433240056037903],\n",
       " [0.8383723258972168],\n",
       " [0.9067519664764404],\n",
       " [0.7188828706741333],\n",
       " [0.9938340902328491],\n",
       " [0.8219002485275269],\n",
       " [0.916765546798706],\n",
       " [0.9125293016433715],\n",
       " [0.17403677403926848],\n",
       " [0.8645563840866088],\n",
       " [0.70514075756073],\n",
       " [0.7169108629226685],\n",
       " [0.320903953909874],\n",
       " [0.8563701748847962],\n",
       " [0.8169689178466797],\n",
       " [0.02198479576036334],\n",
       " [0.8233261346817017],\n",
       " [0.2710165187716484],\n",
       " [0.677165412902832],\n",
       " [0.7391632556915283],\n",
       " [0.4103531211614609],\n",
       " [0.859312891960144],\n",
       " [0.6325321912765502],\n",
       " [0.3670635104179382],\n",
       " [0.43982425332069397],\n",
       " [0.7050966262817383],\n",
       " [0.27812561094760896],\n",
       " [0.38186565041542053],\n",
       " [0.6087178349494934],\n",
       " [0.34419648051261903],\n",
       " [0.2973842442035675],\n",
       " [0.9306874513626099],\n",
       " [0.21399302929639816],\n",
       " [0.9427624583244324],\n",
       " [0.9857752203941346],\n",
       " [0.0047612183028832075],\n",
       " [0.7506929874420166],\n",
       " [0.9078348994255065],\n",
       " [0.8566522240638733],\n",
       " [0.7618463039398193],\n",
       " [0.668631911277771],\n",
       " [0.9241650700569153],\n",
       " [0.8493680834770203],\n",
       " [0.792201840877533],\n",
       " [0.7148325085639954],\n",
       " [0.056762437149882314],\n",
       " [0.8995948195457458],\n",
       " [0.04013048745691776],\n",
       " [0.22909587919712066],\n",
       " [0.9598307847976685],\n",
       " [0.8741104364395141],\n",
       " [0.6533653616905213],\n",
       " [0.09373565055429936],\n",
       " [0.8519721388816833],\n",
       " [0.3056040406227112],\n",
       " [0.7450776815414428],\n",
       " [0.7079944550991059],\n",
       " [0.9916844248771668],\n",
       " [0.5674310505390168],\n",
       " [0.8613269567489624],\n",
       " [0.9080554842948914],\n",
       " [0.5387994945049286],\n",
       " [0.559083878993988],\n",
       " [0.995974349975586],\n",
       " [0.9564558148384095],\n",
       " [0.4544866859912872],\n",
       " [0.43126257359981535],\n",
       " [0.983638596534729],\n",
       " [0.894185996055603],\n",
       " [0.7679749131202698],\n",
       " [0.8337230563163758],\n",
       " [0.3693158388137817],\n",
       " [0.38462031483650205],\n",
       " [0.4537052005529404],\n",
       " [0.8499255180358887],\n",
       " [0.47792385816574096],\n",
       " [0.9663324594497681],\n",
       " [0.8085135102272034],\n",
       " [0.02465898497030139],\n",
       " [0.3053751289844513],\n",
       " [0.6627174973487854],\n",
       " [0.007921078009530902],\n",
       " [0.5163621723651886],\n",
       " [0.8909045934677124],\n",
       " [0.011250876449048519],\n",
       " [0.8415255308151245],\n",
       " [0.9674232602119446],\n",
       " [0.3443384528160095],\n",
       " [0.9896497011184693],\n",
       " [0.9102072596549988],\n",
       " [0.8519070625305176],\n",
       " [0.1283814236521721],\n",
       " [0.4282776236534119],\n",
       " [0.3280437797307968],\n",
       " [0.24065419733524324],\n",
       " [0.9833399176597595],\n",
       " [0.2141224890947342],\n",
       " [0.4004828274250031],\n",
       " [0.9640879988670349],\n",
       " [0.9419660925865173],\n",
       " [0.9922338366508484],\n",
       " [0.14758409261703492],\n",
       " [0.7609422445297241],\n",
       " [0.8326648592948913],\n",
       " [0.5579011619091034],\n",
       " [0.9999050378799439],\n",
       " [0.4412612020969391],\n",
       " [0.7031569480895996],\n",
       " [0.5401381134986878],\n",
       " [0.009639959037303924],\n",
       " [0.3135908663272858],\n",
       " [0.9767229914665222],\n",
       " [0.6306890308856964],\n",
       " [0.4272563338279724],\n",
       " [0.845365560054779],\n",
       " [0.9561593055725097],\n",
       " [0.91402108669281],\n",
       " [0.6940161764621735],\n",
       " [0.07806909345090389],\n",
       " [0.6158438563346863],\n",
       " [0.7225615978240967],\n",
       " [0.9966338157653809],\n",
       " [0.11393985301256179],\n",
       " [0.4642819046974182],\n",
       " [0.5149915337562561],\n",
       " [0.16620794534683228],\n",
       " [0.46297268867492675],\n",
       " [0.7293200016021728],\n",
       " [0.682761150598526],\n",
       " [0.9266647338867188],\n",
       " [0.9489128351211548],\n",
       " [0.8232885360717773],\n",
       " [0.8930569887161255],\n",
       " [0.2564986661076546],\n",
       " [0.9508607864379883],\n",
       " [0.8958199381828308],\n",
       " [0.9931846737861634],\n",
       " [0.6813860297203064],\n",
       " [0.9463648796081543],\n",
       " [0.7497933864593506],\n",
       " [0.6972291946411133],\n",
       " [0.9223033905029296],\n",
       " [0.027424008958041667],\n",
       " [0.2998907744884491],\n",
       " [0.1508325904607773],\n",
       " [0.5826543927192688],\n",
       " [0.9325905680656433],\n",
       " [0.6025421559810639],\n",
       " [0.4872626602649689],\n",
       " [0.22097648680210114],\n",
       " [0.527090972661972],\n",
       " [0.6709523797035217],\n",
       " [0.9787110686302185],\n",
       " [0.3358417093753815],\n",
       " [0.5925040900707245],\n",
       " [0.020779527351260187],\n",
       " [0.9834395527839661],\n",
       " [0.6228326439857483],\n",
       " [0.9209933876991272],\n",
       " [0.8452420949935913],\n",
       " [0.42665920257568357],\n",
       " [0.8699601411819458],\n",
       " [0.6031868100166321],\n",
       " [0.8923734188079834],\n",
       " [0.6635309994220734],\n",
       " [0.10114212036132812],\n",
       " [0.8783905267715454],\n",
       " [0.7229664325714111],\n",
       " [0.8022229313850403],\n",
       " [0.4855096697807312],\n",
       " [0.940567409992218],\n",
       " [0.0612375408411026],\n",
       " [0.7687554478645324],\n",
       " [0.9908536553382874],\n",
       " [0.4724402964115143],\n",
       " [0.6397605180740357],\n",
       " [0.990477693080902],\n",
       " [0.31312375664711],\n",
       " [0.5752409517765045],\n",
       " [0.5487773597240448],\n",
       " [0.8598761677742004],\n",
       " [0.9484375596046448],\n",
       " [0.9628758907318116],\n",
       " [0.9458160877227784],\n",
       " [0.562287175655365],\n",
       " [0.7963934302330017],\n",
       " [0.9062033057212829],\n",
       " [0.78904287815094],\n",
       " [0.9090969204902649],\n",
       " [0.6614536046981812],\n",
       " [0.9709473371505737],\n",
       " [0.9992980718612671],\n",
       " [0.9366612672805786],\n",
       " [0.7017372965812683],\n",
       " [0.4297515511512756],\n",
       " [0.7969090223312378],\n",
       " [0.6456243753433227],\n",
       " [0.9463176608085633],\n",
       " [0.2280159443616867],\n",
       " [0.5290099740028381],\n",
       " [0.5887499809265136],\n",
       " [0.328222393989563],\n",
       " [0.9019125699996948],\n",
       " [0.008588528679683804],\n",
       " [0.8325993537902832],\n",
       " [0.5873705267906189],\n",
       " [0.9098486423492431],\n",
       " [0.9173303365707397],\n",
       " [0.939596438407898],\n",
       " [0.6093039751052857],\n",
       " [0.24344581365585327],\n",
       " [0.850589907169342],\n",
       " [0.5582986295223236],\n",
       " [0.5499050796031952],\n",
       " [0.8260032057762146],\n",
       " [0.7453978419303894],\n",
       " [0.3780445486307144],\n",
       " [0.6757110118865967],\n",
       " [0.7751363277435303],\n",
       " [0.06188492402434349],\n",
       " [0.723967432975769],\n",
       " [0.8882237672805786],\n",
       " [0.8438942193984985],\n",
       " [0.9659858584403992],\n",
       " [0.5877291917800903],\n",
       " [0.6327124118804932],\n",
       " [0.8305283665657044],\n",
       " [0.9461821556091309],\n",
       " [0.8074901819229126],\n",
       " [0.67924724817276],\n",
       " [0.7593995451927185],\n",
       " [0.9790935039520263],\n",
       " [0.6481235504150391],\n",
       " [0.2574830621480942],\n",
       " [0.755344033241272],\n",
       " [0.8672394275665283],\n",
       " [0.9824946880340576],\n",
       " [0.0695067971944809],\n",
       " [0.7732838749885559],\n",
       " [0.4935540914535522],\n",
       " [0.25359861701726916],\n",
       " [0.5355781018733978],\n",
       " [0.7461421728134155],\n",
       " [0.9922186613082886],\n",
       " [0.9702871561050415],\n",
       " [0.8831268906593323],\n",
       " [0.7295219779014588],\n",
       " [0.3428761661052704],\n",
       " [0.14811157435178757],\n",
       " [0.17111262828111648],\n",
       " [0.4194349467754364],\n",
       " [0.854607629776001],\n",
       " [0.24774680137634278],\n",
       " [0.9046958804130554],\n",
       " [0.07352227345108986],\n",
       " [0.6310336589813232],\n",
       " [0.48360095024108884],\n",
       " [0.7037561297416687],\n",
       " [0.7871514797210694],\n",
       " [0.41305527091026306],\n",
       " [0.889261519908905],\n",
       " [0.48936004042625425],\n",
       " [0.6626739978790284],\n",
       " [0.7265278816223144],\n",
       " [0.7435764789581298],\n",
       " [0.3289678305387497],\n",
       " [0.9865960478782654],\n",
       " [0.4663634717464447],\n",
       " [0.03680277392268181],\n",
       " [0.5997050642967224],\n",
       " [0.22072360292077065],\n",
       " [0.8804009079933166],\n",
       " [0.8216241598129272],\n",
       " [0.8548890590667725],\n",
       " [0.5429694592952728],\n",
       " [0.12479572892189025],\n",
       " [0.03250400722026825],\n",
       " [0.10882177650928497],\n",
       " [0.8623334884643554],\n",
       " [0.8436108112335206],\n",
       " [0.6797731518745422],\n",
       " [0.9458759665489197],\n",
       " [0.665927505493164],\n",
       " [0.7509382963180542],\n",
       " [0.9767541527748108],\n",
       " [0.9639537334442139],\n",
       " [0.6218006134033203],\n",
       " [0.7930310606956482],\n",
       " [0.36269287168979647],\n",
       " [0.9764947414398193],\n",
       " [0.45460068583488467],\n",
       " [0.5963759660720825],\n",
       " [0.6298639297485351],\n",
       " [0.8912499666213989],\n",
       " [0.18384204655885697],\n",
       " [0.08703376352787018],\n",
       " [0.8700425505638123],\n",
       " [0.32394221127033235],\n",
       " [0.5208622455596924],\n",
       " [0.9678876638412476],\n",
       " [0.8934535384178162],\n",
       " [0.827895188331604],\n",
       " [0.555486673116684],\n",
       " [0.8673654675483704],\n",
       " [0.6724158763885498],\n",
       " [0.4417645990848541],\n",
       " [0.7948161721229553],\n",
       " [0.8345578074455261],\n",
       " [0.06894158981740475],\n",
       " [0.551091605424881],\n",
       " [0.9046024322509766],\n",
       " [0.8561197400093079],\n",
       " [0.9591331720352173],\n",
       " [0.8402212023735046],\n",
       " [0.886246132850647],\n",
       " [0.9387791395187378],\n",
       " [0.887408459186554],\n",
       " [0.7040445566177368],\n",
       " [0.9727595686912537],\n",
       " [0.8566678881645202],\n",
       " [0.7950203657150269],\n",
       " [0.7919774889945984],\n",
       " [0.9113078594207764],\n",
       " [0.7161463022232055],\n",
       " [0.8590518355369567],\n",
       " [0.3123535752296448],\n",
       " [0.18789959251880645],\n",
       " [0.9614097118377686],\n",
       " [0.953201174736023],\n",
       " [0.6853823065757751],\n",
       " [0.9609213471412659],\n",
       " [0.9466454863548279],\n",
       " [0.9303029894828796],\n",
       " [0.32610404640436175],\n",
       " [0.37579138576984406],\n",
       " [0.578913950920105],\n",
       " [0.7447842717170715],\n",
       " [0.9060702204704285],\n",
       " [0.03283598842099309],\n",
       " [0.9171403169631958],\n",
       " [0.9860191702842712],\n",
       " [0.499215167760849],\n",
       " [0.8343396782875061],\n",
       " [0.39057586193084715],\n",
       " [0.5383441805839538],\n",
       " [0.8558869123458862],\n",
       " [0.21477552354335785],\n",
       " [0.9786963582038879],\n",
       " [0.00876847393810749],\n",
       " [0.6927179098129272],\n",
       " [0.8224761605262756],\n",
       " [0.9405643701553345],\n",
       " [0.877630603313446],\n",
       " [0.44613766074180605],\n",
       " [0.6446654319763183],\n",
       " [0.6108727335929871],\n",
       " [0.7837131977081299],\n",
       " [0.8369121551513672],\n",
       " [0.5963197350502014],\n",
       " [0.9044371247291565],\n",
       " [0.2570266008377075],\n",
       " [0.03387838862836361],\n",
       " [0.7017186880111694],\n",
       " [0.9216246604919434],\n",
       " [0.48865503668785093],\n",
       " [0.9980445384979248],\n",
       " [0.6701369404792785],\n",
       " [0.21750792860984802],\n",
       " [0.4875896632671356],\n",
       " [0.42242527604103086],\n",
       " [0.8326062202453614],\n",
       " [0.7908514022827149],\n",
       " [0.854045820236206],\n",
       " [0.9992302536964417],\n",
       " [0.8983985900878906],\n",
       " [0.8926889657974243],\n",
       " [0.9639059901237488],\n",
       " [0.7317880153656006],\n",
       " [0.7105233907699585],\n",
       " [0.8100751399993896],\n",
       " [0.13168903961777687],\n",
       " [0.9455197215080261],\n",
       " [0.1568020612001419],\n",
       " [0.43567017316818235],\n",
       " [0.916104519367218],\n",
       " [0.8481664180755615],\n",
       " [0.8379402995109558],\n",
       " [0.9499879002571106],\n",
       " [0.9949822545051574],\n",
       " [0.8251403450965882],\n",
       " [0.16749195903539657],\n",
       " [0.20510828197002412],\n",
       " [0.9627455592155456],\n",
       " [0.9937984704971313],\n",
       " [0.40569411516189574],\n",
       " [0.677717673778534],\n",
       " [0.7884353876113892],\n",
       " [0.059276615455746653],\n",
       " [0.7062660217285156],\n",
       " [0.35528404712677003],\n",
       " [0.9652569055557251],\n",
       " [0.4022852838039398],\n",
       " [0.9593745589256286],\n",
       " [0.8223368048667907],\n",
       " [0.31630594432353976],\n",
       " [0.6216100454330444],\n",
       " [0.5148454368114471],\n",
       " [0.8175131916999817],\n",
       " [0.3803166151046753],\n",
       " [0.9489905834197998],\n",
       " ['Invalid SMILES'],\n",
       " [0.6950738430023193],\n",
       " [0.6164662718772889],\n",
       " [0.8609262466430664],\n",
       " [0.809065043926239],\n",
       " [0.958089005947113],\n",
       " [0.6227618217468261],\n",
       " [0.9057432055473328],\n",
       " [0.9276050448417663],\n",
       " [0.9452437400817871],\n",
       " [0.8052318572998047],\n",
       " [0.7956777095794678],\n",
       " [0.88623206615448],\n",
       " [0.5656218886375427],\n",
       " [0.7200149059295654],\n",
       " [0.7929478049278259],\n",
       " [0.7737758159637451],\n",
       " [0.9012328743934631],\n",
       " [0.06468367874622345],\n",
       " [0.8318320870399475],\n",
       " [0.42034990787506105],\n",
       " [0.006659208703786135],\n",
       " [0.10852580070495606],\n",
       " [0.9413922905921936],\n",
       " [0.9574250817298889],\n",
       " [0.9434037566184997],\n",
       " [0.8185638308525085],\n",
       " [0.4692430913448334],\n",
       " [0.19946256875991822],\n",
       " [0.5434209525585174],\n",
       " [0.9089046001434327],\n",
       " [0.35308892130851743],\n",
       " [0.39665263146162033],\n",
       " [0.0529538232833147],\n",
       " [0.8873550176620484],\n",
       " [0.7501974701881409],\n",
       " [0.7732584476470947],\n",
       " [0.754291319847107],\n",
       " [0.7197012424468994],\n",
       " [0.37455909252166747],\n",
       " [0.6475663781166077],\n",
       " [0.7944458365440369],\n",
       " [0.7411375880241394],\n",
       " [0.35424137115478516],\n",
       " [0.7479565978050232],\n",
       " [0.6340273737907409],\n",
       " [0.030795471277087927],\n",
       " [0.9944073677062988],\n",
       " [0.2627196729183197],\n",
       " [0.1723567947745323],\n",
       " [0.9622434854507447],\n",
       " [0.6761602878570556],\n",
       " [0.4925942659378052],\n",
       " [0.9937664031982422],\n",
       " [0.9183100461959839],\n",
       " [0.1918645605444908],\n",
       " [0.0360036700963974],\n",
       " [0.9959065556526184],\n",
       " [0.9775234699249268],\n",
       " [0.2628360420465469],\n",
       " [0.9062671542167664],\n",
       " [0.5455779671669007],\n",
       " [0.6235339641571045],\n",
       " [0.8886768817901611],\n",
       " [0.7158774614334107],\n",
       " [0.8821605324745179],\n",
       " [0.40255093574523926],\n",
       " [0.14164667055010796],\n",
       " [0.8580688595771789],\n",
       " [0.9350645065307617],\n",
       " [0.6049939334392548],\n",
       " [0.10652593821287155],\n",
       " [0.9376121759414673],\n",
       " [0.9823294281959534],\n",
       " [0.7363341927528382],\n",
       " [0.7640586018562316],\n",
       " [0.17008890807628632],\n",
       " [0.013418307807296515],\n",
       " [0.0958818607032299],\n",
       " [0.9436827898025513],\n",
       " [0.05409639738500118],\n",
       " [0.8799456715583801],\n",
       " [0.2748998746275902],\n",
       " [0.8707701206207276],\n",
       " [0.7462768912315368],\n",
       " [0.9701458930969238],\n",
       " [0.8042867183685303],\n",
       " [0.3820167720317841],\n",
       " [0.9029296398162842],\n",
       " [0.8741047978401184],\n",
       " [0.9314334034919739],\n",
       " [0.590065312385559],\n",
       " [0.34873061776161196],\n",
       " [0.028785925544798373],\n",
       " [0.9507649660110473],\n",
       " [0.0029189518303610385],\n",
       " [0.3353016525506973],\n",
       " [0.9882309556007385],\n",
       " [0.0265336150303483],\n",
       " [0.2588864631950855],\n",
       " [0.8030490159988404],\n",
       " [0.5582651734352112],\n",
       " [0.3680850625038147],\n",
       " [0.8529695749282837],\n",
       " [0.34508360028266905],\n",
       " [0.9807688355445862],\n",
       " [0.8072399258613586],\n",
       " [0.05354243814945221],\n",
       " [0.3346437633037567],\n",
       " [0.45566110610961913],\n",
       " [0.959785544872284],\n",
       " [0.40099434554576874],\n",
       " [0.7835545897483825],\n",
       " [0.5380258142948151],\n",
       " [0.7934300541877747],\n",
       " [0.7507274389266968],\n",
       " [0.6439135551452637],\n",
       " [0.9231572151184082],\n",
       " [0.86764497756958],\n",
       " [0.9540668845176696],\n",
       " [0.9542232632637024],\n",
       " [0.968222439289093],\n",
       " [0.5272262275218964],\n",
       " [0.7913387537002563],\n",
       " [0.34706307053565977],\n",
       " [0.7716165781021118],\n",
       " [0.8606856822967529],\n",
       " [0.8551103949546814],\n",
       " [0.6247834801673889],\n",
       " [0.32564069926738737],\n",
       " [0.4943760335445404],\n",
       " [0.5616411507129669],\n",
       " [0.6023477792739869],\n",
       " [0.9341645240783691],\n",
       " [0.6729963183403015],\n",
       " [0.5247708469629287],\n",
       " [0.7938953995704651],\n",
       " [0.8387571215629578],\n",
       " [0.7529542922973633],\n",
       " [0.9914179921150208],\n",
       " [0.7885467529296875],\n",
       " [0.7271699070930481],\n",
       " [0.4419921517372131],\n",
       " [0.5721385598182678],\n",
       " [0.2396051362156868],\n",
       " [0.9339887738227844],\n",
       " [0.9954105377197265],\n",
       " [0.8987524747848511],\n",
       " [0.596892261505127],\n",
       " [0.9903755187988281],\n",
       " [0.848427951335907],\n",
       " [0.9935833573341369],\n",
       " [0.8109907746315003],\n",
       " [0.6352495074272155],\n",
       " [0.40713430047035215],\n",
       " [0.8578776121139526],\n",
       " [0.694005012512207],\n",
       " [0.7504082560539246],\n",
       " [0.9118583083152771],\n",
       " [0.7878989338874817],\n",
       " [0.6526770114898681],\n",
       " [0.09792619496583939],\n",
       " [0.5403764486312866],\n",
       " [0.8984039664268494],\n",
       " [0.8103973746299744],\n",
       " [0.6869933366775512],\n",
       " [0.5639776289463043],\n",
       " [0.5727658629417419],\n",
       " [0.997415566444397],\n",
       " [0.9370393395423889],\n",
       " [0.9279873490333557],\n",
       " [0.1465242773294449],\n",
       " [0.18233427107334138],\n",
       " [0.9450308322906494],\n",
       " [0.9969864249229431],\n",
       " [0.8497971177101136],\n",
       " [0.7630790114402771],\n",
       " [0.5006036043167115],\n",
       " [0.7580788254737854],\n",
       " [0.9662200927734375],\n",
       " [0.7053202390670776],\n",
       " [0.24805806577205658],\n",
       " [0.030973594263195993],\n",
       " [0.5908826470375061],\n",
       " [0.930668044090271],\n",
       " [0.23349981904029846],\n",
       " [0.6729315757751465]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nextAID",
   "language": "python",
   "name": "nextaid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
